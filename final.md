<style>
ul{
margin:0;
}
img{
margin:auto;
}
</style>

<div style="position:fixed; top:0; left:0; margin:10px;">
<a href="#contents">Back to Contents</a>
</div>
#Orange Street
<a name="contents"></a>
##Contents

[toc]


<a name="client-interaction"></a>
##Client Interaction 
[Back to contents.](#contents)

###Aims
Our client interaction brought to light these main issues to consider in our project. 

**They do not want to think about the noise:**   
The user talked a lot about having to worry about remembering specific times they were caused inconvenience by the noise outside and then provide evidence of those inconveniences. Therefore the project needs to store data and visualise it in a way that could be presented as evidence of their inconvenience. Also from this we can gather that the user does not really want any real interaction with the device as it would mean them thinking about the noise. The point of the device should be that it runs autonomously (to create peace of mind for the user) collecting data for the user to visualise and use as evidence at their discretion.

**Solution needs little to no interaction, running in the background:**  
The user expects the device to have a maintenance level of a device akin to a router. They expect some general maintenance but they do not want to be too involved with it, ie worrying about battery life all the time.

**A long term solution:**  
Along the lines of the other two themes before, the user wanted the system to be robust and sustainable. During the meeting the user said that this year so far wasn't as bad as last and didn't want to complain this year if it was to change next and get worse. Therefore the end result of the project should be something that the user can maintain on their own.

**People should be aware of their own noise levels:**  
The client talked a lot about wanting the people in the street to know how noisy they are being. Points being brought up suggesting signs that had the decibel reading on them so that people can visualise the noise like a road sign that turns on when you are going too fast. Although, as we wouldn't be able to enforce the sound level it could possibly encouraging even challenging people in the street to be loud to try and make the sign go off.

####Meeting ONE

We had to speak with the clients before we started to formulate a solution. We had to know what they wanted, so that we could make a solution that best filled their needs. To prepare for this we researched proper interviewing techniques by reading: *“Interviewing users: How to Uncover Compelling Insights (Steve Portigal, 2013)”*.  

The main lesson we took from the Portigal book was that we needed to let the client talk. We did not want to steer their answers in any direction. We organised roles amongst ourselves, with a leader asking questions and the rest taking notes. If the note takers had any questions we made sure they were asked at the end of the question so that the client did not lose track.

Our intended outcome for this meeting was to have an incite into our clients perception of the problem they are having, whereby we can devise a method in which to solve said problem. We had an idea of the client's problem before we went into the meeting, based on what we were told in the project brief. Therefore, in preparation we made a list of questions that we felt we needed answered to gain a better understanding. We then trimmed down our question list intending to allow our client to talk as freely as possible. 

All recordings, notes and questions from the meeting can be found [here](meeting_one/)
####Meeting TWO

We spoke to another client that was meant to be present at the original meeting but was unable to make it. They are a landlord of Orange Street tenants living next to a popular club called ‘The Ballroom’. In this meeting we followed the same layout as set in the first meeting, making one person take the lead and the others taking notes, we also followed the same structure of questions.

All recordings, notes and questions from the meeting can be found [here](meeting_two/)

![](/Images/OrangeStreet_dist.jpg)

####Other Communication

We also tried to speak to other people, pubs and bars in the area to try and gain a greater understanding of the issues that were present in the street and what people were doing to try and solve them.  

A message was sent out to three of the main purported culprits of noise on the street: The Maori Bar, The Seven Stars and Ballroom. The message said: 
>Hello, three other Kent students and I have undertaken a project whereby we are looking into sound around Orange Street. We were wondering how you deal with sound i.e. keeping it contained or if there is a way you try and stop it from spilling into the street. 
>
>Thank you  

The Maori Bar responded with: 
>Hello Matthew thanks for your message, our building is soundproofed in the walls and we make sure the door is kept shut so to trap and muffle as much of the sound as possible. We also monitor our sound levels regularly with a decibel reader.  
>
>Orange Street is a very busy street especially on a Friday and Saturday night and attracts a lot of foot traffic. Of course telling people to keep quiet is near enough impossible as they pass between venues. We don't have live music or DJ's but The Seven Stars and The Ballroom do. We are a small venue with only about 40 people in at one time on average. We would be interested in your findings in your project. Keep us posted.
>
>The Maori Bar team.

This shows that there is an awareness that the street is loud but it can be hard to police outside of the venue. It was also interesting that they mentioned keeping the door closed as the door being opened and closed with the occasional noise leaking out was cited as a large source of annoyance by the clients.  

The Seven Stars allowed us to come and talk to a manager. They said they maintain the noise by trying to keep the doors and windows shut as much as possible and they have a set of double doors so that the sound is muffled when people are coming and going. They also said that they had decibel readers that they checked to make sure that the inside noise was in accordance with the set sound levels. They know that people are noisy when they leave the establishment but he said there was nothing that could be done about that as it is a problem that comes with the clientele.  

Unfortunately The Ballroom did not get back to us.  

####Noise Law

We looked into noise laws. We did this so that we could understand, firstly, if the way we were collecting data was legal and to see if the data we provide to the client would be of any use to them.  

The main thing we found were that there are noise laws in place but they only apply to residential premises according to the ***Noise Act 1996 - Non commercial only. This law only relates to noise that is coming from a residential premises such as neighbours, it does not affect licensed premises.***  

We also found that police can close a premises based on the ***161 Closure orders for identified premises - (1) A senior police officer may make a closure order in relation to any relevant premises if he reasonably believes that: (a) there is, or is likely imminently to be, disorder on, or in the vicinity of and related to, the premises and their closure is necessary in the interests of public safety, or  (b) a public nuisance is being caused by noise coming from the premises and the closure of the premises is necessary to prevent that nuisance.***

The following email is from the Environmental Health department of Canterbury County Council. This is the department that handles any noise complaints regarding both residential and commercial premises. We enquired to them about noise laws.

>Dom  
>
>In response to your enquiry each premises is based on its own merits, hours of operation and location so we don’t have any set levels for Canterbury District. Two documents which may be of use to you are the Control of Noise at Work  
>
>Regulations 2005 which deals with operators requirements for their staff and BS:4142
>  
>The Method for rating and assessing industrial and commercial sound - this looks at how and where to measure sound in mixed residential and business communities. I hope this helps  
>
>Tricia  

The first document mentioned is in relation to the noise levels that are acceptable for their staff and people working on the premises rather than those sounds that are heard from outside the premises so this is of less of a concern to us. Although, it can still be found [here](http://www.legislation.gov.uk/uksi/2005/1643/contents/made). 

The other document mentioned is owned by the British Standards Institute and incurs a cost if we wish to use it. A brief description of the document said the standard is used by the council and describes methods for rating and assessing:

* Helps assess sound levels at proposed new residential premises 
* Enables the investigation of complaints by determining sound levels 
* Reduces the likelihood of financial penalties 
* Supports current UK planning guidance and Environment Agency guidance sound of industrial or commercial nature.

We also looked into licensing reviews and found that anyone with evidence suggesting there is a problem can request a licence review from the council as long as it comes under one of the following:

* Crime and/or disorder 
* Public nuisance 
* Public Safety 
* Prevention of harm to children

In this case, the noise generated from the pubs and or clubs would be classed as a nuisance. 

When making a review request evidence needs to be provided to support the case that you are making. This evidence can be in multiple forms such as:

* a diary or record of events or incidents 
* photos or video evidence
* sound recordings
* a record of complaints made to the responsible authorities about the premise

##Initial Ideas

At this point we had spoken to enough individuals to have a good place to start working on a potential prototype. We needed to plan how we would efficiently tackle the problem ahead of us. We first had to make a list of requirements based on our aims. We needed:<span class="todo">link to aims</span>

* Multiple sensors 
* The sensors to be self-sustainable for a long period of time.
* The sensors to be out of the way but effective (Out of mind, not reminders to the clients), so no wires running everywhere.
* A way to visualise all the data from these sensors into a form that was easily understandable
* Control over the network, and ideally a way to configure it.
* Cases that could ensure the endurance of the devices in different conditions.

Translating this to a solution we can work with:

* We were looking at some form of wireless solution, we could not afford to have wires running everywhere when the clients wanted the solution ‘out of mind’. With wireless comes many different solutions, we investigated the best options available to us.
* We know we had to sample sound - we did not know how often to sample however so we went to investigate that also.
* A microphone is needed with any other electronic circuitry that comes with it. 
* A way to display this data, we decided to investigate ways to do such a thing.
* We had to ensure some form of data backup also.
* Some of the requested locations for these sensors were completely unreachable by permanent power supplies, which led us to the investigation of long term battery solutions.
* We needed a case that could survive harsh weather and conditions, one that would be of a suitable Ingress Protection Rating.




<a name="sensor"></a>
## Sensor
[Back to contents](#contents)

###Description

The sensor samples sound every minute. The microphone in the sensor starts collecting sound data every minute it produces data that represents a sine wave. The data is then put through an ADC (Analogue to Digital Converter) that amplifies the analogue data (sine wave) and removes any voltage noise. It is then analysed by the sensor, taking fifty points along the wave, in this minute time period, to find the amplitude of the wave (how loud the sound is).  

The final sensor is comprised of multiple parts, [Microphone](#mic), [ADC](#adc), [Board](#sensor_board), [Clock](#sensor_clock), [Battery](#battery), [XBee](#sensor_xbee), [Case](#sensor_case):

**<a name="mic"></a>Microphone**

The microphone is an electret microphone with a built in MAX4466 amplifier. This amplifier has adjustable gain which is used to boost the raw signal which is passed from the microphone. In the final version of this sensor we decided that this amp was not enough and that we needed to add another amplifier to the circuit to improve our recordings

**<a name="adc"></a>ADC**

The circuit contains an external ADC in the form of an ADS1115. This was chosen for three main reasons. The 16 bit resolution allowed us to work with a larger range of values which we would not have got using the onboard ADC of the board. It also contained a programmable amplifier which we could use to further amplify the signal coming from the mic. The other reason was that it allowed comparison between 2 analog inputs. 

We found that we were getting a lot of noise coming from the mic which we discovered was caused by the voltage from the board. To counter this voltage coming out of the board with the data coming back from the microphone on the ADC. This allowed us to get the difference and remove any electrical noise from the mic.

**<a name="sensor_board"></a>Board**

The final board we chose was the Rocket Scream Mini Ultra 8 MHz Plus. The reasons this was chosen are below:

 - Battery connector
 - Built in charging circuit
 - Low power voltage regulator
 - Thermal regulation for the battery
 - Relatively cheap
 - Battery voltage monitor

 We slightly modified this board by removing the power indicators to decrease the current draw. 
 
 The board is in a near constant sleep state and only wakes up on a pin interrupt to take readings and send the data back to the sensor. This ensures a battery life of around one and a half months. The interrupt is triggered from the clock.
 
**<a name="sensor_clock"></a>Clock**

The sensor needs a clock to be able to accurately record the time readings are taken. The clock on the sensor also has a further purpose of waking the sensor up at certain times. For this we used a DS3231 which kept time incredibly accurately and also provided an 'alarm' function which could pull a pin high or low based on a set of programmable rules. We set this rule for every minute which means it pulled the pin low every minute. We then had the board sleep listen for an interrupt on that point. 

**<a name="battery"></a>Battery**

We used a 1500mah lithium polymer battery for the sensor as it provided a good balance between power and size. In previous iterations we used a 2000mah which provided a longer battery life however we couldn't source another one in time for the final deployment.

**<a name="sensor_case"></a>Case**
The Case was designed in a way that was intended to aim our microphone at the noisy street and protect the electronics from the elements. It was 3D printed at a high fidelity (high infill of plastic) with a thickness of 3mm as not to allow water in through the printed plastic. It was then sanded and sprayed with a high fill primer to fill any pores left int the plastic left from the printing process. As the case had to be closed around the electronics of the sensor a seam was built in that was filled with a neoprene strip as to stop any water from getting through said seam.

A final version of the code can be found [here](sensor/micTest).

Below is a diagram of the final circuit used.

![6](Images/sensor/diagram.png)


### Previous Work

####Iteration 1 - Researching Hardware
Based on our [client interaction](#client_interaction) we decided that we had to make a device that measured the volume of the sound in Orange Street, collecting the data and sending it back to a server so that it is stored and can be accessed by the client to use.

Building on the initial ideas we had and also looking back to the client interaction section we decided on the following requirements for the sensor:

- Long battery life
- Reliability
- Record the volume of sound in the surrounding area
- Transmit that data back to a hub


We looked into how sound works and discovered that we would need to capture a sound wave and subtract the minimum from the maximum to get an overall amplitude of the wave. We could then use this data to calculate other things such as decibels but initially recording the sound wave was crucial. We planned to use a microphone and microcontroller board to select some points on a sound wave and perform the calculation.

**The Microphone Amplifying Circuit**

After talking to the client the important part of the diagram would remain constant throughout the process, which was the amplifying circuit. For a microphone to be able to produce a voltage signal able to be processed for data it must be amplified, we know for certain we are measuring noise levels in this project and so this is a crucial step.

![1](Images/sensor/IMAGE1.PNG)

The basic place to start is a non-inverting amplifying circuit, used with any op-amp it effectively calculates the gain based on two resistor values going into an inverting and non-inverting input. (Gain= 1+ (R2/R1)

The OP-AMP IC we’ve been using is the MCP 6002, the datasheet can be found here. (http://ww1.microchip.com/downloads/en/DeviceDoc/21733j.pdf)

![2](Images/sensor/IMAGE2.PNG)

It’s an IC with two OP-AMPS and isn’t designed for anything too complicated, for the time being it’s perfect to get a basic amplifying circuit built.

Looking at the microphone itself, it’s a very basic condenser microphone that a lot of products (Mostly cheap ones) use in manufacturing. For the time being it’s ideal, however later on we might consider modifying the microphone as this can have great benefits without changing much of the circuit let alone power requirements (Direction, pop filters, windscreens etc).

Most microphones that feed into a amplifying circuit are biased by a resistor value and then thrown down to ground (0v), one could use a potential to guarantee the voltage level applied to a microphone also.

So far, we’re looking at a circuit like this.

![3](Images/sensor/IMAGE3.PNG)
Other solutions that can be found on the web include using a different IC (As opposed to the MCP 6002) and modifying the circuit above. 

List of other IC’s and amplifiers we looked into.

<table>
	<tr>
		<td>IC Chip</td>
		<td>Link</td>
	</tr>
	<tr>
		<td>NE5535, TL071, OPA 371</td>
		<td>http://www.zen22142.zen.co.uk/Circuits/Audio/lf071_mic.htm</td>
	</tr>
	<tr>
		<td>LM386</td>
		<td>http://www.learningaboutelectronics.com/Articles/Microphone-amplifier-circuit.php</td>
	</tr>
	<tr>
		<td>MCP 6002</td>
		<td>http://www.aiscube.com/main/downloads/RVHS/RV_lesson_301112.pdf</td>
	</tr>
</table>

Although some may seem more suitable than others, as of right now we’ve focused on having a working circuit. In the long term, once we are happy with this solution we will most likely have a pre-built circuit instead of designing the non-inverting amplifier ourselves and choosing which OP-AMP to use. 

**Processing the Data**

The next step was to process the data, there were many ways to handle this from Microcontrollers to all-in-one IOT boards. We researched the following methods of taking data from a microphone to then process.

<table>
	<tr>
		<td> Potential Idea </td>
		<td> Description </td>
		<td> Pros </td>
		<td> Cons </td>
	</tr>
	<tr>
		<td> IOT Microcontroller Boards </td>
		<td> A whole board designed to handle multiple functions, such as Wifi/3G/Bluetooth with sensors, a built in microcontroller and more.</td>
		<td> 
			<ul>
				<li>A lot of built in features to reduce complexity of building communications between chips.</li>
				<li>Widely available, often designed for IOT solutions.</li>
			</ul>
		</td>
		<td> 
			<ul>
				<li>Often a lot of unneeded accessories (such as temperature sensor)</li>
				<li>Increase the physical size of the device based on unneeded extras</li>
			</ul>
		</td>
	</tr>
	<tr>
		<td> Microcontrollers  </td>
		<td>Singular processors, adjustable clock speeds, do not have the functionality of some of the features present on a microcontroller.</td>
		<td> 
			<ul>
				<li>A lot more freedom in deciding how to run the device.</li>
				<li>Removes any unnecessary features.</li>
				<li>Allows to scale down size of the sensor considerably.</li>
			</ul>
		</td>
		<td> 
			<ul>
				<li>Longer to produce and build together as can take more components.</li>
				<li>More complicated, requires further electronics knowledge.</li>
			</ul>
		</td>
	</tr>
	<tr>
		<td> Barebones  </td>
		<td>Using only modules to handle the work, avoiding any form of major processing.</td>
		<td> 
			<ul>
				<li>Potentially reduce power usage.</li>
				<li>Removes any unnecessary features of a Microcontroller.</li>
				<li>Allows to scale down size of the sensor considerably.</li>
				<li>A lot more freedom in deciding how to run the device.</li>
			</ul>
		</td>
		<td> 
			<ul>
				<li>Longer to produce and build together as can take more components.</li>
				<li>Much more complicated, requires further electronics knowledge.</li>
				<li>Difficult to prototype a sensor without a Microcontroller.</li>
			</ul>
		</td>
	</tr>
</table>

For the purpose of a prototype we decided to work with a IOT Microcontroller, we need to investigate common IOT Microcontroller modules, ones that are ideal for our implementation of this IOT device. 

**Arduino Microcontroller Boards**

Arguably one of the most popular development boards commercially available, has a full function IDE written in C++. Multiple different boards designed for different purposes, all having sharing the basic functionality (such as analog inputs) while offering unique differences. They allow for shields to be placed into them which add even further adaptability, allowing for 3rd party hardware to be interfaced easily into the device. This functionality can be as simple as an SD card reader, a WiFi chip or even an external clock. 

Which one suited our best needs? What did we need in a device?
<ul>
	<li>Low power</li>
	<li>Within reasonable price range</li> 
	<li>Relatively small</li>
	<li>Ability to interface with certain components (Backup mediums if network is down, an external clock to keep track of time, an ADC input and form of digital output to transfer data to a wireless module).</li>
</ul>

This narrowed down our choices to the following Arduino devices.
<ul>
	<li>Arduino Nano</li>
	<li>Arduino Pro Mini</li>
	<li>Arduino Macro</li>
	<li>Arduino Uno</li>
</ul>

![4](Images/sensor/IMAGE4.PNG)

**Arduino Nano**

![5](Images/sensor/IMAGE5.PNG)

(https://www.arduino.cc/en/uploads/Main/ArduinoNanoManual23.pdf)

“The ATmega168 has 16 KB of flash memory for storing code (of which 2 KB is used for the bootloader); the ATmega328has 32 KB, (also with 2 KB used for the bootloader). The ATmega168 has 1 KB of SRAM and 512 bytes of EEPROM (which can be read and written with the EEPROM library); the ATmega328 has 2 KB of SRAM and 1 KB of EEPROM” (Arduino.cc, n.d.)
<ul>
	<li>Serial: 0 (RX) and 1 (TX). Used to receive (RX) and transmit (TX) TTL serial data. These pins are connected to the corresponding pins of the FTDI USB-to-TTL Serial chip.</li>
	<li>External Interrupts: 2 and 3. These pins can be configured to trigger an interrupt on a low value, a rising or falling edge, or a change in value. See the attachInterrupt() function for details.</li>
	<li>PWM: 3, 5, 6, 9, 10, and 11. Provide 8-bit PWM output with the analogWrite() function.</li>
	<li>SPI: 10 (SS), 11 (MOSI), 12 (MISO), 13 (SCK). These pins support SPI communication, which, although provided by the underlying hardware, is not currently included in the Arduino language.</li>
	<li>LED: 13. There is a built-in LED connected to digital pin 13. When the pin is HIGH value, the LED is on, when the pin is LOW, it's off.</li>
</ul>

The nano is a small device that has all the functionality that we ideally would want, it has connections over Serial, I2C and SPI (Although supported by hardware, not supported by Arduino libraries).

It requires a minimum of 5v operating power, anything below and functionality is lost and we run the risk of disabling features.

It can run the ATMega 168 or 328, we would ideally use the 328 as it offers much more space (EEPROM, SRAM and Flash memory) and is a later iteration over the 168. The dimensions of the device are 0.73” x 1.70”.

**Arduino Pro Mini**

![6](Images/sensor/IMAGE6.PNG)

(http://www.atmel.com/images/Atmel-8271-8-bit-AVR-Microcontroller-ATmega48A-48PA-88A-88PA-168A-168PA-328-328P_datasheet_Complete.pdf)

Essentially the Pro Mini is identical to the Arduino Nano except for the added ability of lower bootloader space and the ability to run at 3.3v over 5v and other small differences that do not add much to our required project.

“There are two version of the Pro Mini. One runs at 3.3V and 8 MHz, the other at 5V and 16 MHz... The ATmega328 has 32 kB of flash memory for storing code (of which 0.5kB is used for the bootloader). It has 2 kB of SRAM and 1kBs of EEPROM.“ (Arduino.cc, n.d.)

**Arduino Uno**

![8](Images/sensor/IMAGE8.PNG)

The Uno sticks out in this comparison due to its size difference against the previous 3, which begs the question - why then? Simply put, the Arduino Uno is a very friendly board to use, and for prototyping would be ideal as we would not need to worry about many problems that we could face when going straight in with one of the other solutions. It also shares a lot of common ground with the other 3, except for its size.

It runs the ATmega328P, which has slight differences to the ATmega328 (Slight power reduction). Only uses 0.5KB of the Flash memory for the bootloader, runs at 16MHz and needs at least 6v operating voltage which like the macro can cause instability if run at this low a voltage. 

The biggest benefit for us, was that the Uno would offer easy adaptability and help quickly work with a prototype while we decide which microcontrollers to use, their frequency, and work on breadboards instead of soldering straight away. 

**MBED FRDM-K64F**

![9](Images/sensor/IMAGE9.PNG)

Another popular developer of IOT boards, using ARM based architecture instead of AVRs. The argument between these two processor architecture is often put down to ARM is powerful, and AVR is not so much. There are variants on the processors but otherwise they tend to stick to those groups. MBEDs have an online compiler and IDE, which works in a similar fashion to Arduinos but is effectively always online which comes with its own problems such as requiring internet access. 

The most ideal MBED board we found was the FRDM-K64F which is regarded as the flagship board, it’s perfectly ideal for prototyping - but in the long run most likely would be too much of a power killer. It’s compatible with most Arduino shields too. In the long run however, the FRDM-K64F while having many features, especially when combined with the MBED application shield - is too much functionality for what we need bundled down, even in a prototype we do not need such complexity. 

Even with all its functionality switched off the device consumes more amps than one of the arduino boards. However we decided that this board would be ideal if used for our hub, as during that period power will not be a concern. 

####Iteration 2 - Making our own amplifier
#####Issues with previous iteration
No issues to report, this is the first design iteration.

#####Result of iteration
Using our amplifier we’ve been able to start sampling sound. We’ve been experimenting with the FRDM K64F and Arduino Uno in measuring sound. Our experiments with these boards consisted of testing the circuit we had made for measuring sound levels. Using the FRDM-K64F:

~~~c++
Serial serial(USBTX, USBRX); // Serial connection
 
// Initialize a pins to perform analog input
AnalogIn   ain(A0);
 
int main(void)
{
    while (1) {
        // print to serial analog input
        printf("normalized: %d \r\n", ain.read_u16()); 
        
    } 

}
~~~

The FRDM-K64F has given us superb accuracy when sampling the microphone, values ranging from 0-65555. This is due to the 16bit analog to digital converter on the board. 

![11](Images/sensor/IMAGE11.PNG)

We also sampled using an Arduino Uno:

~~~c++
int analogPin = 3;     // Microphone amp connected pin 3
int val = 0;           // variable to store the value read

void setup()
{
  Serial.begin(9600);          //  setup serial
}

void loop()
{
  val = analogRead(analogPin);    // read the input pin
  Serial.println(val);             // print to serial the value
}
~~~ 

We need a way to take these values and use them to sample the sound wave. The values are being sent over serial, so having a program listening on serial and processing data will let us be able to visualise sound over time. 

 Using Java we have written a program that talks on serial to the boards. Our program samples 50 times in a second, long enough to gauge a sound wave. Using the minimum and maximum samples we calculate the range and pass all these values along to a file to be saved.
 
This is the section of our code which samples at 50 times a second:

```java 
	// Read values from sensor

        /* Calculate the range of 50 samples */
        int i = 50;
        int max = data[0];
        int min = max;
        int range;
        int j;

        while (i-- > 1) {
            j = data[i];
            if (j > max) {
                max = j;
            }
            if (j < min) {
                min = j;
            }
        }
        
        range = max - min;
        
        // Save to file 
        // ...
```

Logging this data shows the structure and accuracy behind our sensor. Using this data we can increase the sensitivity of our sensor or increase the sample rate to gain a better understanding. 

As of right now the values (0-500) on the Y axis are not too useful for us, we can determine whether noise has risen but we ideally want to work with decibel levels. 

To view our testing results in more detail, please see here. 

####Iteration 3 - Using a pre-built amplifier
#####Issues with previous iteration
The size of the amplifier we built was ideally too large for a small sensor, we could condense it by soldering and moving components closer together but it would be easier and more efficient to buy pre-built amplifiers. 

#####Result of iteration
We’ve purchased a pre-built amplifier to simplify our circuit, ideally we don’t want wires going everywhere and using a prebuilt amplifier makes our task easier due to less complexity and time required to build one. 

![12](Images/sensor/IMAGE12.PNG)

The next step is to wire the pre-built microphone amplifier to one of the boards, we’ve decided to use the FRDM K64F for the time being as its sample range from 0-65555 makes it appealing to work with.
 
![13](Images/sensor/IMAGE13.PNG)

This was a very simple change and nothing too complicated occurred, but it does benefit us in the long run. The size of the new amplifier works in our favour as its size makes it very easy to adopt into a system where as previously we had a cluster of wires and components. It doesn’t risk being disconnected when compared to our previous amplifier which was held together through loose wires. 

* For a version of this code see <a href="/sensor/initialNoiseLevel/sketch_dec02a/sketch_dec02a.ino">here</a>.
* For the Java sampling program see <a href="/SensorJavaSamplingCode/">here</a>. 

####Christmas Deployment
Over the christmas period we deployed a version of our sensor that wrote data to an SD card. This was designed to go into one of the resident's houses and record data for a short period of time. We used an arduino uno with a shield that contained an SD card reader. 

We needed a way to power the whole board for the time that we would be away over the christmas break. A standard Uno with no low power code drew a current ~42.5mA. A standard AA battery would provide around 1500mAh.

 $$ 1500mAh/42.5mA=35.29 hours $$

The christmas break lasts 4 weeks so 35.29 hours was going to be nowhere near enough and we would only get data for the first day and a half. 

We discussed several other options that included rechargeable batteries, multiple AA batteries and plugging it into the wall. Multiple AA batteries was discounted because it would be very expensive and we’d need a lot of them. It would also increase the size drastically.

Rechargeable batteries looked like a viable option as they came in much higher capacities and would allow the client to recharge them if they did happen to run out when we were away. We did some initial calculations with the 10000mAh battery that we had to hand:

$$ 10000mAh/42.5mA=232.94 hours $$

$$ 232.94/24= 9.7 days $$

While this was getting better it was still slightly too short for what we needed it for. At this point bigger batteries became more expensive and the University wouldn’t be able to order them in time. This meant we had to make the arduino use less power when it was running. To do this we used low power libraries and turned off everything that we weren’t using. After doing this we managed to get the power usage down to ~22mA. 

$$ 10000mAh22mA= 454.54 hours  $$

$$ 454.54/24= 18.9 days $$

This was a more reasonable amount of time and would give us a good amount of data that we could use in the future. The battery pack we were using could also be easily charged using  a micro usb cable (the same kind that is used to charge phones) which meant we could ask the client to charge it if it did run out of power.

A version of this code can be found [here](sensor/SDCardPrototype/)

We collected a significant amount of data from this and a graph of the levels we recorded is included below. You can also find the raw data [here](sensor/cdata.csv)

![](Images/sensor/graph.png)


####Iteration 4 - Results from Christmas deployment
#####Issues with previous iteration
There were several issues that arose with the code and hardware we created during the christmas testing We discovered that the mic signal was not being amplified enough which led to a lot of readings being the same even though the noise levels were vastly different. The previous iteration also recorded data directly to an SD card for us to look at later. This is an issue as we needed some way of transmitting data back to the board.

#####Result of Iteration
We struggled with amplifying the sound due to also amplifying a lot of electrical noise. We added an XBee module to the board so that we could transmit data back to the hub which worked without any issues.

####Iteration 5 - External Clock and XBees
#####Issues with previous Iteration
We noticed some issues with the previous iteration cutting off data after sending large amounts. This is an issue that needs fixing and is caused by the internal hardware of the XBee module (see [Networking](#networking) for more details). We are also currently getting the time for timestamping the data from the internal clock. This is proving to be an issue as the time is not accurate. Over longer periods the time on the board will drift further and further away from the actual time. Another issue is if the sensor runs out of battery the time will be lost.

#####Result of Iteration
The loss of data when transmitting was due to the XBee buffers being overloaded as we sent the data too quickly. To solve this we added small delays in between sending the data. This fixed the issue and the data appears to be being sent without any issues. We also added a clock to the circuit. This has a backup battery so it can still keep time in the event of the sensor losing power. This clock is also accurate and can keep to +/-1 second over a year. 

####Iteration 6 - Fixing sensitivity of the Microphone
The aim of this iteration was to fix the issue in iteration 2 where we discovered that microphone readings were not being amplified enough to pick up changes in the noise level.

#####Issues with previous Iteration
No issues

#####Result of Iteration
We added a 16-bit ADC (Analog to Digital converter) which gives us a higher resolution and also allows us to remove electrical noise using a comparison of two pins. To do this we used a potential divider to half the 3.3v signal that the board was running off and put in pin 1 of the ADC, we then put the microphone output into pin 2. Comparing these 2 pins gave us a wave that was far less noisy, as the power voltages were effectively cancelling each other out. This means that we could produce a wave which had an maximum amplitude that was the same as our resolution. Previously there was a noise baseline which when amplified increased, leading to us not being able to amplify it too much. These changes allow us to see the noise level change in much more detail and also pick up smaller changes.

####Iteration 7 - Rechargeable Batteries
For this iteration we wanted to add a rechargeable battery to the circuit so the client could charge it in their house without having to buy standard alkaline batteries. We also wanted to think about low power.

#####Issues with previous Iteration
No issues

#####Result of iteration
We changed our board from an Arduino Uno to a low power version called the 'Rocket Scream Mini Ultra 8 MHz Plus'. This draws a much lower current than the Arduino due to it's more power efficient on board regulator. This board also comes with a battery connector which allows us to plug a lithium polymer battery into it and provides pins which a source of up to 20V can be plugged in. We began testing by charging it using a standard usb charger and currently it functions fine, albeit slowly as the charging circuit can charge the battery using a max current of 500mA. We also tried a different version of the board which used even less power, the 'Rocket Scream Mini Ultra', however we decided against using this due to lack of features such as as voltage regulator, which we would need to use our battery efficiently and recharging circuits.

####Iteration 8 - Lowering the Power
In this iteration we wanted to make sensor run at a lower power so it could last on batteries for as long as possible.

#####Issues with previous Iteration
Our testing program was running on the board constantly which led to the battery not being able to fully charge as the code was making the board use a lot of power. This led us to think there was an issue with the charging circuit and look into that. The issue in the end was fixed by making sure the program didn't run and then leaving it to charge for a longer period of time.

##### Result of Iteration
We used the [Rocket Scream Low-Power](https://github.com/rocketscream/Low-Power) library which allowed us to turn off all of the functions of the processor we were not using and put it into a deep sleep mode. This deep sleep dramatically reduced power usage. We are using the watchdog timer on the board to allow us to set the amount of time we sleep. However the watchdog timer can only count up to 8 seconds so to get it to sleep for 1 minute, we needed to run it 8 times. Some example code for this can be seen below:

```c++
#include <lowpower.h>
#include <avr/wdt.h>

bool count = 8;

void setup()
{
	//Clear the prescaler
	WDTCSR |= (1<<WDCE) | (1<<WDE);

	//1001 for the watchdog prescaler is 1024k cycles 
	//which is ~8seconds
	WDTCSR = 1<<WDP0 | 1<<WDP3;
	
	//Enable interrupt
	WDTCSR |= _BV(WDIE);

}


//Watchdog timer interrupt
ISR(WDT_vect)
{
  if(count > 0)
  {
  	//decrement the count 
  	count--;
  }
  else
  {
  	//Do nothing which returns to the main loop
  }
}

//Put the processort to sleep until it's woken up by WDT
void sleep()
{
	LowPower.powerDown(SLEEP_FOREVER,ADC_OFF, BOD_OFF);  
}
void loop() 
{
    // read the data...
}  
```
####Iteration 9 - Another new clock
##### Issues with the previous iteration
There were several issues we noticed after the previous iteration. One of these problems was that the watchdog timer wasn't accurate enough and microphone sample timestamps were drifting a lot. We also found that the code sometimes failed and just stopped working completely. The XBee was drawing a large amount of power as it was running in transmit mode all of the time, even when it was not being used by the program.

#####Results of iteration
We changed the clock to a DS3231 which allowed us to fire interrupts at predetermined intervals, such as every minute. We could then use that wake the processor from it's sleep and it could then take the readings. We made some modifications to the DS3231 by removing the power and transmit LEDs to reduce power consumption. The clock ensured that readings were taken at the same point every minute. We also reprogrammed the Xbee to use pin hibernate mode. This meant that when we pulled a pin low on the board, the Xbee would also enter sleep mode. This sleep mode reduced it's power usage down to around 500μA which was acceptable. The Xbee was then only powered on when we wanted to send data, which was a tiny fraction of the time. Changing to interrupt based processor sleep seems to have solved the problem of the board not waking from sleep.

All code mentioned in this section can be found [here](sensor/)<a name="networking"></a>
##Networking 
[Back to contents](#contents)
###Description

Our network is a robust low powered mesh that has a coordinator handling as many routers and end points as we require. The coordinator is capable of addressing each node on the network, with the nodes only ever addressing the coordinator. The hardware used to handle interaction on the network is the XBee S2 module using the ZigBee protocol and communicating to our devices using serial. XBee S2 have sleep functionality and only draw 40mA upon transmitting making them ideal for a low powered solution. 

Due to the configuration behind each XBee module we were able to have full control over our own network as well as control features such as sleep modes and addressing. We are using API mode when handling XBee modules on our network, as this firmware allows us to have create and format packets to the style we require. The network lets us determine useful information from traffic such as delivery status, fragmentation and assembly of packets. 

The sensors were programmed in C++ and the Hub was programmed in Python. We have written two Libraries to be able to communicate in the format the XBee modules expected while working with these languages. Using these libraries offer us the full potential of API mode and allow control over a mini network of low powered wireless devices.

The libraries were designed so that one line of code would be required to send a message across the network while abstracting all heavy processing on traffic. The benefit of this was that the library could change and not require a whole body of code to change for the other sections of the project.  

Hub API:

~~~python
# Send message with API, sensor1 lets the API know which 64bit
# address we're looking for, returns successful or not
response = xbee.sendMessage("sensor1", "Hello world!")
~~~

Sensors API:

~~~c++
void helloWorld(){
	// Send message with API, will transmit to the coordinator
	// Returns 0 if successfull
	int response = xbee.sendMessage("Hello world!"); 
}
~~~

![13](Images/Networking/IMAGE14.PNG) 

###Previous Work

####Iteration 1, Researching Wireless Solutions

#####The Solution we need

Based on how we aim to solve the problem, with multiple sensors sending data back to a hub. <span class="todo">reference initial ideas page</span> Our networking solution needs to allow us to have a hub that can have multiple sensors connected wirelessly. The network needs to allow us to communicate data reliably. The data we are expecting to be sending between the sensors and the hub is only going to be ≈ 2000 bits an hour based on the fact that we are going to send a timestamp and an averaged hourly value as a 'long' and an 'int'. 

#####Researching Technologies

Because we decided that we were working with a wireless network based on our aims<span class="todo"> link to why we chose to use wireless</span> Investigating potential solutions considerations of strength, distance, maximum payload size and power usage have to be made. The most obvious solution is WiFi.

Here is a table that we formulated over common wireless solutions: 
 
<table>
	<tr>
		<td></td>
		<td>Range (line of sight)</td>
		<td>Range (Urban)</td>
		<td>Frequency</td>
		<td>Current Consumption (max)</td>
		<td>Power Consumption (sleep)</td>
		<td>Voltage</td>
		<td>Data Rate (s)</td>
	</tr>
	<tr>
		<td>WiFi</td>
		<td>100m</td>
		<td>20m</td>
		<td>2.4GHz/5GHz</td>
		<td>300mA</td>
		<td>Varies</td>
		<td>>= 5v</td>
		<td>Varies</td>
	</tr>
	<tr>
		<td>Bluetooth</td>
		<td>100m</td>
		<td>20 - 30m</td>
		<td>2.4GHz</td>
		<td>30mA</td>
		<td>Varies</td>
		<td>N/A</td>
		<td>1 - 3Mbits</td>
	</tr>
	<tr>
		<td>Bluetooth Low Energy</td>
		<td>100m</td>
		<td>< 100m</td>
		<td>2.4GHz</td>
		<td>15mA</td>
		<td>Varies</td>
		<td>N/A</td>
		<td>1Mbit</td>
	</tr>
	<tr>
		<td>XBee XSC</td>
		<td>9500m</td>
		<td>610m</td>
		<td>902 - 928MHz</td>
		<td>60mA(R) 265mA(T)</td>
		<td>45uA</td>
		<td>3.3v</td>
		<td>10Kbits</td>
	</tr>
	<tr>
		<td>XBee Series 2</td>
		<td>120m</td>
		<td>40m</td>
		<td>2.4GHz</td>
		<td>40mA (R & T)</td>
		<td>< 1uA</td>
		<td>3.3v</td>
		<td>250Kbits</td>
	</tr>
</table>

Values taken from [Wifi vs Bluetooth power consumption](http://science.opposingviews.com/bluetooth-vs-wifi-power-consumption-17630.html), [Bluetooth power consumption](http://www.digikey.com/en/articles/techzone/2011/dec/bluetooth-goes-ultra-low-power), [XBee power consumption](https://www.sparkfun.com/datasheets/Wireless/Zigbee/XBee-Datasheet.pdf).


<br>
**WiFi**

WiFi is widely used and accepted as a way to wirelessly transmit data. This means our clients should be familiar with it in some sense. However WiFi is not really intended to be used in devices that need to be situated in one place for an extended amount of time. This is because WiFi can use a lot of power when sending and receiving data. Hence why smartphones can lose power quickly while connected to a WiFi network.

Because we shouldn't be sending anything more than ≈7 kilobytes a day, therefore the ability to send upwards of 10 megabytes might be overkill. 

However, the main reasons why we would want to use WiFi comes in two forms. Not only can we transmit reasonable distances but we can directly connect devices to the clients home WiFi.<span class="todo"> do we want to link to where the client said we could use wifi?</span> This could, in theory, lead to the elimination of the hubs which would result in problems regarding data processing. The sensor would have to do all of its data processing onboard meaning it could effect the timing of the capture of our data.

One of our mains goals for the sensor is to make the battery as long as possible . WiFi is one of the more power consuming options, so using WiFi with or without a hub our sensor would have to use much more power when receiving and transmitting data making the battery life less than desirable. 

Another one of the more problematic issues of WiFi would be the encryption, the data we are sending is not of national security nor anything that could be any real use to anyone aside our clients and in the event that our clients change their WiFi credentials (Encryption key, SSID etc) then we risk jeopardising the sensors.

<br>
**Bluetooth and Bluetooth Low Energy**

Bluetooth is also a viable alternative for transferring over a low bandwidth where speed is not too key an issue. Depending on how often we schedule the device to transmit data. Since the range on this is considerably lower (5-30 meters) so we would have to consider integrating this with a hub of some kind to forward our data. Fortunately some of the properties across Orange Street feature flat rooftops; upon which we could attach peripherals such as antennas if we need to.  

Bluetooth smart devices have a typically very low sleep current which equates to low power consumption when it isn’t being used. This is ideal as the device will have periods of inactivity once we decide upon which kind of timeframe it should be operating over. 

Bluetooth works on a dynamic network topology called PAN, which supports up to 8 other devices and a minimum of two, although we don’t plan on having an abundance of sensors in one house (Minimum most likely 3) 

<br>
**XBee S2 and XSC**

We found that the S2 in particular was more than adequate for our desires, having one of the lower current draws for transmitting/receiving data, especially that of in sleep with a good data rate (250kbps) and working on a mesh network topology.  Not only this, but the XBee offered full configurable settings on its usage and setup, allowing us greater control of the network than other alternatives.

The XBee can also be programmed manually to work on its own meaning in theory we could eliminate the Microcontroller entirely, however this solution leads to problems involving working out the current time and large packet payloads. We may come back to it at a later point, but for now we decided to use it purely for as means for A-B for our data. 

#####Results of Iteration

We've decided to choose the XBee S2 as our solution. Due to its customisability and low powered nature its perfect for having our own control on a network. If we utilised WiFi for example then we would have to concern ourselves with high power usage and lacking full control of the network. The XBee offers a multitude of different possibilites and fits closest to our goals of a low powered networking solution. 

####Iteration 2, XBee AT Mode

XBees have microcontrollers onboard that store and control the instructions that let them know where data is being sent, sleep functionality, node hopping, retry attempts and much more. For our network we needed to configure each XBee to work within our desired parameters.

In order to configure these settings we required software and hardware to interface into the XBee, software created by Digit International called XCTU. We had to make our own programmer however as we hadn't received our own programmer as of yet. With this we have started altering the settings on the firmware to adapt the XBees to our desired network structure.

![5](Images/Networking/IMAGE5.PNG)

When programming the XBees over serial, there are many different options for installing new firmware settings. We've been working with ZNet 2.5 AT for both coordinators and end routers on the network as this is the recommended starting firmware as provides all functionality we currently need. Although we could end changing at a later date depending on what functionality we require.

XBees share one trait across all networks, that is the requirement for them to be able to communicate; using a PANID. The PANID is a 64 bit integer that is unique on a network and separates other networks in close proximity from each other, unless you unluckily both choose the same PANID - however with the options available that is a very slim chance.

![6](Images/Networking/IMAGE6.PNG)

Using AT Command mode we've had to specifically set values on the XBee. These have ranged from 64bit destination address to encryption being enabled. This information is used in creating packets, we can't change this information without reprogramming the XBees which could cause a problem. However we know which nodes need to address each other in the long run, so none of these details need changing for the time being. 

After we set the two XBee devices to be on the same personal network (sharing PANID), aligning their firmware (ZNet 2.5 AT), and finally setting them as coordinator and router - they were able to communicate. In AT mode we could send bytes down serial to the XBee and the firmware of that XBee would create a packet based on what we’ve set as predefined instructions. The XBee constructed these packets based on their programmed firmware, which we had configured. 

We ideally need one hub per house which could communicate and route data between nodes. The hubs are going to be powered by mains as opposed to sensors which are powered by batteries and in varying locations. The sensors communicate using XBees to the Hub on a mesh topology, in theory allowing us to have as many sensors as we may need.

#####Setting up AT Mode

We’ve been using AT mode so far with XBees. AT Mode is designed to be very straightforward to use, the device connects to the XBee module on serial and sends any data in a form of bytes to be used in a packet and transmitted on the network. We’ve been using XCTU to talk between XBee modules, making sure we understand the concepts of how they are designed to communicate and parameters for addressing each other. 

![7](Images/Networking/IMAGE7.1.PNG)

Setting the XBees up involves us requiring two major pieces of information, the PANID and Address. The coordinators destination points to 0x000000000000FFFF while any end points or routers need to point their destinations to the coordinator's address. That can be achieved in two ways, one way is using 0x0000000000000000 or actually specifying its address.

![7.2](Images/Networking/IMAGE7.2.PNG)
#####AT Mode and Hardware

We’ve started using FRDM-K64Fs to talk to one another as they provide an application shield and libraries alongside which are full compatibility with a hardware ‘installation’ of the XBee (installation being the XBee has dedicated pins, rather than soldering it to the board). The K64Fs had simple programs designed purely to send data to one another.

![0](Images/Networking/IMAGE0..PNG)

#####Testing Range

We know roughly the distance of an XBee from its datasheet, however our clients home and in particular Canterbury has very old structures. These structures have very thick walls, we need to know whether our XBees can transmit through these obstacles or whether we need to boost the signal. We modified the previous code used to test between FRDM K64Fs to calculate latency between packets and then walked around parts of campus determining if we had enough packet loss for concern.

We used ‘The Shed’ as the coordinators base station while moving through different parts of the School of Computing while the coordinator was making note of whether it was receiving any data or not. We’ve mapped our findings and calculated distances to determine whether we are going to struggle with walled structures or not.

![4](Images/Networking/IMAGE4.PNG)

As shown above the XBees are capable of travelling around 30m and only suffer complete packet loss when passing multiple walls, which is most likely unavoidable in our project. We will definitely need some form low powered communications network so boosting the power will be a trade off we most likely can’t afford.

######Results of Iteration
 
The FRDM-K64F is the current choice for the Hub but not for the sensor, so at some point we will need to test the capability of using an FRDM-K64F to talk to an Arduino style board. This has furthered our understand of using XBees with AT mode, specifically how to progress further and utilise these modules in our future components. 

* <a href="/MBEDXbeeTest/receiver.cpp">Code for FRDM-K64F receiver</a>
* <a href="/MBEDXbeeTest/transmitter.cpp">Code for FRDM-K64F transmitter</a>

####Iteration 3, AT Mode with Hub, Sensor and Clock

Our next role is to implement the Hub, Clock and the sensor(s) on the network. For this to work all nodes need to be able to communicate with one another.

![7.3](Images/Networking/IMAGE7.3.PNG)

The sensor is planning to send around 700 bytes of data an hour to the hub, the hub then processes that data. If the clock sends a request to the hub then the hub processed that and responds with data back to the clock. Using AT mode it is simply a matter of writing down to serial the bytes you wish to send across the network, all the nodes are configured to talk to one another. The clock and sensor both have their destination addresses set 0x0 pointing to the coordinator while the coordinator has its address set to 0x000000000000FFFF. This allows it to broadcast, the sensor won’t respond as it will not be needed and will be utilising sleep mode on the XBee whereas the clock will respond as the broadcast will directed at the clock.

#####Problems

In theory the solution was fine, however a problem we did not account for was payload size. For unknown reasons the coordinator would lose segments of incoming data and then just carry on until it received the next transmission. This meant that data was becoming malformed and mixed up which meant it was completely useless to us. Upon further investigation, we found that the internal buffers of the XBees are that of 202 bytes. The XBee datasheet offers flow-control through the use of pins CTS and RTS but in fact it's very likely the XBee doesn’t support fragmentation of packets as we haven’t found anything to prove it does. Using the CTS and RTS pins could prove interesting, but it would only solve the problem of knowing when the XBees internal buffers are almost full or empty.

So we’re likely to be able to send around 150 bytes in a single packet (RF data and packet header), we circumvented this for the time being by purposely delaying and breaking up the data the sensor had. This took advantage of the internal hardware on the XBee which has timers dedicated to calculating when to stop listening on serial, form a packet and send it. By breaking the message down we effectively caused the XBee to send multiple packets across the network in one go.

The other solution to this problem could be to transmit more frequently, every minute over an hour but battery usage would be the main concern. 

A concern was raised regarding the issue of multiple sensors, as of right now we have no way of knowing who is sending data to the hub as well as data becoming malformed and merged due to AT mode. With one sensor this won’t be an issue:

![7.73](Images/Networking/IMAGE7.73.PNG)

However with multiple sensors, data becomes inoperable:

![7.75](Images/Networking/IMAGE7.75.PNG)

Making this an issue to tackle in the next iteration.

For more information on these particular systems and how they process data see their respective sections.

######Results of Iteration
AT mode worked well for our initial task however we need to solve the problem of large payloads and multiple sensors as these will become more apparent later in our project. With this iteration passed we have successfully created the prototype project as a whole, the network allows for all components to successfully communicate with one another.

For information regarding the datasheet for the XBee S2, see this document.
https://www.sparkfun.com/datasheets/Wireless/Zigbee/XBee-Datasheet.pdf
* Page 12 refers to data input buffers of size 202 bytes
* Page 11 refers to ‘Packetization Timeout’ from serial to RF

Code used for Hub with AT Mode can be found <a href="/XbeeAPI/Hub/HubAT.py">here</a>.

####Iteration 4, API Mode 
#####Issues with previous Iteration
On our previous iteration we discovered a few problems with using AT mode and the XBee modules themselves. We can’t fragment packets without having some form of intervention ourselves and we can’t determine who is sending data which will cause issues for multiple sensors. A simple fix to this is to prefix all incoming data with an identifying name and delimiter, for example clock:”message”. However this is most likely overhead as further research has shown that by using API mode we won’t need to do this as source addresses are passed with packets. We did manage to confirm that fragmentation is not possible on XBee S2 networks, see www.digi.com/wiki/developer/index.php/Determine_MTU for details. 

#####API Mode

API mode is the more advanced for AT in a lot of ways. The XBee will function the same and you can transmit data to the XBee in the same manner, that’s serial down to the XBee RX pin. However, just sending a stream of bytes won’t make the XBee transmit data. With API mode you have to make the packets as opposed to having them made for you. This has required a lot of research into formats and to do so accurately with good use of examples, we used “Building Wireless Sensor Networks” by Robert Faludi. Faludi provides excellent examples and technical details on the use of API mode.

![7.4](Images/Networking/IMAGE7.4.PNG)

API mode has many different packets you can create, from investigating the formats of these packets we can see specifically how to form and send data. We would only need 3 different packets format for our system to be fully utilised, those of:

* 0x10 TRANSMIT REQUEST
* 0x90 RECEIVE PACKET
* 0x8B TRANSMIT REQUEST

The typical format of one these packets, for example the transmit request appears as follows:

Transmit (“Hello world”): “7E 00 19 10 01 00 00 00 00 00 00 00 00 FF FE 00 00 48 65 6C 6C 6F 20 57 6F 72 6C 64 D5”
Receive (“Hello world”): “7E 00 19 10 01 00 00 00 00 00 00 00 00 FF FE 00 00 48 65 6C 6C 6F 20 57 6F 72 6C 64 D5”
Status (“Hello world”): 7E 00 07 8B 01 00 00 00 00 00 73

These hexadecimal values while alien looking are relatively straightforward to identify as again they are following a format. The packets we will form will be a transmit packet while the packets we receive will be a status and receive packet. 

The transmit packet must follow this format when being created:

![7.5](Images/Networking/IMAGE7.5.png)

The receive packet will follow this format, so we know the offset of each byte:

![7.6](Images/Networking/IMAGE7.6.png)

The status packet will follow this format, again we know the offset of each byte:

![7.7](Images/Networking/IMAGE7.7.PNG)

These formats are important because when we wish to send or receive data to the XBee it will need to be in this format. Instead of:

~~~python
self.serial.write(“Hello world!”) # AT method
~~~

We would use:

~~~python
self.serial.write(bytearray.fromhex('7E 00 17 10 01 00 13 A2 00 40 C1 FD 49 FF FE 00 00 48 65 79 20 57 6F 72 6C 64 A7')) # API method
~~~

However for true implementation we would want this to return some indicator of whether the message was transmitted successfully or not. For example:

~~~python
response = transmit(bytearray.fromhex('7E 00 17 10 01 00 13 A2 00 40 C1 FD 49 FF FE 00 00 48 65 79 20 57 6F 72 6C 64 A7')) # API method
if response == x:
       # do something ...
~~~

If we had this, then we could use this within our existing components to implement a form of error awareness and resolution.

#####API Mode System Benefits
API mode offers a lot more in utility at the cost of more effort on implementation. The benefit of using it, is that you can access all data in a packet header. This would allow us to see all sorts of ranging information from source address to checksums and would solve our problem of needing to identify nodes on the network. It also comes with a lot of added benefits, using API mode we could determine whether a node is out of range, what nodes are on the network and whether packets have been successfully delivered or not. 

For API mode to be implemented we need to build two libraries capable of handling the underlying functions of the XBee. One library would need to utilise Python and the other C++, the Python library would be used by the Hub while any other nodes would use the C++ library. The Hub had its own requirements and so the library would need to reflect that.

######Hub API requirements
* Node discovery (Locate any new nodes)
* Heartbeat requests (Determine whether a node is still active)
* Individual node addressing (Address any node on the network as opposed to broadcasting each time)
* Fragmentation (Break large payloads down)
* Assembly (Reassemble large payloads)
* Message storing (If multiple sensors are sending their payloads at the same time, the hub needs to be able identify and reassemble separate messages at the same time)
* Status awareness (Was a packet received?)

######Sensor and clock API requirements
* Fragmentation (Break large payloads down)
* Assembly (Reassemble large payloads)
* Respond to heartbeat requests (Respond to hubs request to see if they are ‘alive’)
* Single message storing (The hub will only ever send one message, to the clock it will be requests or a heartbeat but never at the same time. To the sensor it will be a heartbeat.)
* Status awareness (Was a packet received?) 

######Multiple Sensors Solution
Initially we used AT mode for working with one sensor, however problems soon arose when we planned to add multiple sensors to our network. The problem was that it would become impossible to identify who was sending data causing different sensor readings to become mixed up across transmissions. Using one sensor was fine because only one source of traffic with sensor readings was expected, the clock wouldn’t interfere as this was a different format of data. Using API mode has let us identify each node on the network and where each stream of traffic is coming from, fixing this problem for us.

![7.76](Images/Networking/IMAGE7.76.PNG)

######Fragmenting Packets
As show in our previous iteration the XBee didn't support packet fragmentation, so in order for this to work we need to manually number each frame in a packet from each source. We also need to be able to determine which is the final frame of a packet, we’ve decided to use an ‘!’ character, which would never naturally appear in any of our normal packets.

![7.8](Images/Networking/IMAGE7.8.PNG)

######Arduino String vs Char Array
When working with the Arduino IDE its been very tempting to use the 'String' class. A class that offers useful functionality similar to that of Java. With it we can work out the length of a string, split it, concat it and do general operations on it. However, this functionality comes as the cost of efficiency. In C, strings don't exist in the format of other languages rather they exist as char arrays and are handled very differently. Arduinos IDE dumbs down the char array into a string. The lack of efficiency comes at the cost of memory, with such a small amount of RAM, the nodes could potentially crash if we are not careful. 

For us, we want total efficieny above all else. Although it might be easier to implement a string over a char array, we have decided with the later. 

~~~c++
  char char_array_hello[20] = "Hello world!"; // Using char array to store sentence
  String string_hello = "Hello world!"; // Using String to store sentence
~~~

#####XBee Sleep Settings
Amongst many of the settings available on the XBee, sleep is a must have for our sensors. There are many options available to us when configuring sleep mode. Most importantly how often does the module stay asleep for and then how often to stay awake for. In terms of reserving power this feature is invaluable for the sensor. 

The sensor currently uses a set of pins on the XBee to command it to enter sleep mode, or awake from sleep mode - thus limiting its power consumption.

######Results of Iteration
These requirements will allow us to build a robust network capable of recovery upon failure, if a packet isn’t received for example then retransmit it. We will be able to also provide more feedback to the client such as if a node is no longer within range, if they moved the sensor too far away from the Hub for example.

* For implementations of our node code (C++), see <a href="/XbeeAPI/Nodes">here</a>.
* For implementations of our Hub code (Python), see <a href="/XbeeAPI/Hub">here</a>.

####Iteration 5, Implementing API Mode

#####Sensor out of range

![10](Images/Networking/IMAGE10.PNG)

Building off the last iteration, we can now offer more information to the client and to the network. This includes node discovery, a very powerful feature for our network which determines whether nodes are now out of range or have disappeared. Using this information we could alert the client that the sensor has run out of battery and thus died or been moved too far away from the Hub.  

We decided to construct a dummy sensor in order to demonstrate range testing and how to show the client this information in a meaningful manner. The dummy was created using an Arduino Uno, a set of LEDs (Green and Red) as well as a XBee breakout board. It was transmitting random floating values from one of its analog pins in the same format expected of the actual sensor. The sensor was initially given a set of LEDs; green and red. These LEDs would turn on or off depending on the circumstance, if the sensor was within range of the coordinator (The hub) the green LED would light up, else if the sensor was out of range the red LED would light up. 

Although simple in principle, this was not possible with the use of AT mode (Without doing some serious and inefficient modifications). Using status packets we can determine whether a sensor was within range or not and then use this information to alert the client. We’ve decided that this information could be made easier to understand if the Hub was to alert the web server when a sensor was out of range, as this information can be displayed on the website for easier access rather than flashing LEDs.

* For implementations of the dummy sensor code (C++), see <a href="/XbeeAPI/Nodes/DummySensor/XbeeAPI.cpp">here</a>.

#####Library for coordinator (Hub)

The hubs library is much more extensive than the nodes library. It has to offer the hub the ability to address any node, as well node discovery and heartbeats. These features are cruicial in order to maintain network stability and determine missing nodes, which can reported back to the webserver. 

For more information regarding the Hub and API mode, see [**Hub**](#hub).

#####Library for nodes (Clock and Sensors)

The library created for the nodes on the network was simpler than that of the coordinator. The nodes only need to talk to the coordinator on the network as opposed to each other. The coordinators address is constant, set at 0x00, which means no complicated addressing or node discovery.

######Packet Fragmentation and Assembly

Packet fragmentation and assembly are supported along with responding to heartbeats automatically when received. This library will only support one stored message however, it only expects one message at a time from one node - the coordinator. Response codes and transmission status packets are available with the library allowing for us to determine successful packet transmission.

######Different Serial Ports

The actual serial connection on the clock and sensor are different, the Flora board has two serial ports whereas the Rocket Scream only has one. This meant that the serial connection the Flora board used was different to the Rocket Scream and they send data down serial in different ways. To accomodate this the HardwareSerial class was used a reference in the API, allowing us to choose which serial port we wanted the API to use. 

For the Rocket Scream, initialising the API can be done as:

~~~c++
XbeeAPI xbee(&Serial, 0, "test");
~~~

Whereas for the Flora:

~~~c++
XbeeAPI xbee(&Serial1, 0, "test");
~~~

The library handles this construction as follows:

~~~c++
XbeeAPI::XbeeAPI(HardwareSerial *pserial, const char* name) : name(name)
{
  serial = pserial;
  serial->begin(9600);
  serial->setTimeout(1000);
}
~~~

######Polling

The ATMega architecture struggles with some forms of interrupt handling, due to this storing packets when data is received becomes difficult. When a packet is received the XBee will store it internally and wait to send over serial, it can only send when the board is ready to receive. The sensors are often asleep and won't be listening on serial for incoming data, except for when they wake up. When they wake up to sample sound, they can call a method called 'poll' in the library. This method reads in everything it can from serial and then forms messages for the program to use. This method is designed to bypass the problem of interrupt handling and work within the parameters of a device that may need to be sleeping for a long period of time.

An example of polling:

~~~c++
// awakening from sleep...
// take a sample
bool messageAvailable = poll(3); 
if(messageAvailable){
	// respond, read etc ...
}
~~~

The parameter passed to the poll function relates to how many times the program wants to poll, each call waits 250 milliseconds before reading everything it can in from serial. The example above would poll 3 times and would take 750 milliseconds to execute, not including time for atomic operations. 

The poll function:

~~~c++
bool XbeeAPI::poll(uint8_t timesToPoll)
{

    unsigned char input[INPUT_SIZE + 1]; // Total length of data expected, including + 1 for termination \0
    unsigned char packet[INPUT_SIZE + 1]; // Total length of data expected, including + 1 for termination \0

    // How many times we want to check for incoming data
    for(int i = 0; i < timesToPoll; i++){
    	
      // Clear arrays with 0
      memset(input, 0, INPUT_SIZE+1);
      memset(packet, 0, INPUT_SIZE+1);
      delay(250);
      // Read in all data we can up to INPUT_SIZE and store in 'input'
      uint8_t size = serial->readBytes(input, INPUT_SIZE);
      input[size] = 0; // Final termination of array \0


      unsigned char pos = 0;
      unsigned char checkPacket = 0;   
      for(int i = 0; i < size; i++){
        if(input[i] == 0x7E){
          if(!checkPacket){
            checkPacket = 1;
          // If the packet is valid, we want to continue checking serial data for other packets
          }else if (validatePacket(packet)){
            memset(packet, 0, INPUT_SIZE); // Reset packet
            pos = 0;
            checkPacket++;
          // If invalid packet and not checking packet then break 
          }else{
            break;
          }
        }
        packet[pos++] = input[i];
      }

      if(checkPacket == 1){
        validatePacket(packet);
      }

      if(message != NULL && message->hasTerminated()){
        return true;
      }else{
        return false;
      }
  }
    }
~~~

#####Results of Iteration
With our recent iteration we've managed to successfully test the sensor out of range, create working libraries to interface with our nodes without changing any existing code to the systems. Overall this iteration has been a massive success towards the structure of the network and ensuring the strength of it. 
<a name="hub"></a>
##Hub

[Back to contents.](#contents)

![](Images/Hub/IMAGE1.PNG)

The Hub is the coordinator of the network. Its the middleman to all incoming and outgoing traffic from the sensor or clock to the webserver. It will backup data if the network fails at any point, to determine network failure it utilises the libraries provided by the Networking solutions (see [Networking.](#networking)). Using these libraries the hub can maintain a robust network and offer these services:
* Node Discovery, can determine when nodes disappear or appear and assign them 'nicknames' that can be associated to that node. (e.g "sensor on the balcony")
* Fragmentation and Assembly of large payloads, large payloads can be fragmented and reassembled when transmitted across the network.
* Heartbeats, keeping up to date with each node and knowing their current status. Fits into node discovery.
* Transmission reports, for each packet sent a status packet is returned. With this the hub can identify whether a transmission was successfully or not.
* If the network fails the hub will backup any data is has locally and any data it is currently trying to transmit until the network connection has been re-established.
* Will forward all sensor data to the webserver for further processing, if possible otherwise saves locally.
* Forwards requests from the clock for most recent 24 hours of sound averages and will transmit back the response. If an error occurs will transmit an appropriate error code.

The hub is comprised of multiple parts: [Board](#hub_board), [Communication / XBee](#hub_xbee) and [Case](#hub_case).

**<a name="hub_board"></a>Board**
The Hub uses a Raspberry Pi Model B+ running Raspbian Jessie Lite, the Pi offers GPIO pins to connect external boards to it. Using these pins, an XBee module is connected on serial and provides the Pi with its position on the network as coordinator. The Pi only requires three connections for it to function, an ethernet connection, the serial connection to the XBee and finally power. The programs controlling the network are written in Python 3.

**<a name="hub_xbee"></a>Communication / XBee**
The XBee module is configured as coordinator on the network, giving the Hub its status and control on the network. The XBee can address any other XBee module on the network or broadcast to all of them. All other XBees address the coordinator as it is the centre point of the network. Sensors forward their data through the XBees to the Hub the Clock makes requests to the Hub also.

**<a name="hub_processing"></a>Processing Role**
It handles data coming in from the sensor and requests from the clock. The clock can request decibel averages of the past 24 hours using the Hub as a middleman, the Hub then forwards this request to the web server and returns the result to the clock. The sensors submit their sampled data to the hub in order for this to then be sent forward to the web server. 
The Hub takes into account that it may not be able to reach the web server for various reasons, and will try multiple times to connect. If it fails while transmitting sensor data it will save this on the SD card in the Pi, if it cannot request data for the clock it will return an error code instead. 

Upon a series of failed attempts, once a successful attempt is made the Hub will transmit all previous stored data and delete it afterwards to clear space in memory. 

**<a name="hub_case"></a>Case**
The case was a 3D printed design that was required due to the extra components that the Hub required. The Pi has many off the shelf cases that can be used, however due to our requirement of fitting an XBee module these cases would not suffice. The 3D printed case was capable of fitting the XBee module as well as the Pi. The case needed to house the Pi for general concerns of deprecation over time, we didn't want the client to accidently knock a Pi off the shelf without some protection.


###Previous Work and Initial Premise
Unlike the sensor, power consumption was not an issue as the client told us that we could connect to a power outlet. It didn't need to be outside the clients premisses either. This meant we could use any feasible board for this role. We needed a board that could offer the most useful functionality towards our project.

The hub was required to be a middleman between sensors and the web server, forwarding traffic onto the website over ethernet and handling any heavy processing. Initially we planned on using an FRDM-K64F board due to familiarity and easy access to them within the University. 

###Hub Hardware
####Iteration 1 - Researching Hub Solutions
#####FRDM K64F

![](Images/Hub/IMAGE5.PNG)

The FRDM-K64F board has a lot of unneeded functionality. It has an unnecessary amount of sensors on the board itself (temperature sensor and accelerometer for example) which wouldn't add to our project benefits. Although as previously stated we are testing this board because of its availability and our familiarity with it. We are familiar with this board and we know that it has a shield that has the ability to interface with an XBee which is what we decided to use for our networking. <span class="todo">(insert link to networking decisions)</span> Using the board would not be an issue, as we have had extensive skill in handling and programming it. Including the MBED Application shield would benefit us in providing pins designed for an XBee module to interface with. It also offers an LCD display for reporting information back to the client, which could be useful for showing basic messages. However the shield does offer a lot of useless additions as well. Introducing more sensors and obstructing every pin on the FRDM makes it unlikely to be a realistic option for our Hub.

<p class="todo">Here I was going to include my case design implementation for the hub, but it went on for a while, would you like me to do that in its own page?</p>

###Research Into other Boards

####Arduino Uno

![](Images/Hub/IMAGE3.PNG)

Considering the Arduino Uno for the hub as a likely candidate for the fact that the board itself does not have any sensors that would be considered unnecessary like on the FRDM K64F. It is programmable in C much like the K64F so will essentially use the same code. The main reason for choosing this board would be to trim the unessential things from our current solution. The Uno is also a well known board that is vastly documented. 

Unlike the K64F the Uno lacks an ethernet port built in. To remedy this we would have to add an Arduino shield capable of offering ethernet such as the Arduino Ethernet shield. The shield while similar to the MBED Application shield provides the ability to transmit more than just data along ethernet, it could provide power too, although this means adding the PoE (Power over Ethernet) component. This added functionality means the possibility for less wires, this means easier instillation for the client as only one connection would be required. There are libraries that exist to help use the shield and it’s functionality and the board offers a lot of useful debug information regarding current status with sending data, making it easier to work with.

With the problem of ethernet solved this only leaves connection to XBee out. In order to fix this we would have to either, include another [shield](http://uk.rs-online.com/web/p/products/6961670/?grossPrice=Y&cm_mmc=UK-PLA-_-google-_-PLA_UK_EN_Semiconductors-_-Semiconductor_Development_Kits&mkwid=s8484M9Xf_dc|pcrid|88057061283|pkw||pmt||prd|6961670&gclid=Cj0KEQjwid63BRCswIGqyOubtrUBEiQAvTol0WdagHobLZ9zO5iXOsR0-jdPUrM43OJ-dTZv86HIMcgaAkHy8P8HAQ) that had a breakout for the XBee or physically wiring up an XBee. Wiring up an XBee would require soldering the required pins on the XBee to wires that we could plug into the headers of the ethernet shield. If we choose to have the ethernet shield with the PoE module the XBee shield would probably not fit and therefore we would have to solder the XBee to the board. In circumstance of soldering the XBee it would also lead to being unable to then modify the firmware settingson the module without desoldering first leading to more spendature of time. Otherwise we could use the shield on its own, meaning we could reconfigure the XBee at any time.

![](Images/Hub/IMAGE2.PNG)

####Arduino Yun

![](Images/Hub/IMAGE0.PNG)

The Arduino Yun is a very unique Arduino board, as it offers two processors. The AR9331 handles a Linux distribution while the ATMega32U4 handles the board. This means we can run an Operating System with all the benefits that brings on this board. The board comes with an ethernet port and WiFi as well, making it immediately more ideal than the previous two boards mentioned. The board also comes with an SD card port for supplying the Operating System, so in theory a large SD card would allow data logging and more storage in general. The board itself can run Arduino sketches which can interface with shell scripts running on the Linux distro, although the two processors are kept separate; bridging is possible due to a library provided.

However the Yun lacks a great deal of hardware support in terms of volatile memory only offering  64MB of DDR2 ram with 16MB of flash, 9 of which is taken by the Linux distribution. With this considered a better alternative would be something like a Raspberry Pi which could offer more memory and more Operating Systems varieties. The price of a Yun is higher than previous entries, averaging around £50 which is more than double the price of an Uno. 

####Micro Server

![](Images/Hub/IMAGE4.PNG)

It is plausible to use a Micro Server in place of the Hub. The server could have a serial programmer connected to a XBee and use programs to read and access the data coming in. Using a Micro server would give huge benefits in terms of processing power, data storage and security. We could have our own choice of operating system and hardware. Data could come in and be backed up internally, then processed to be sent off. However price and size could cause issues, as these servers do not often come cheap and are a lot larger than other potential solutions. They can also become quite loud and considering noise is what we are trying to help our client with it is probably not an ideal solution furthermore it would draw a lot more power than a development board meaning it could have a visible cost impact on the client.

####Raspberry Pi

![](Images/Hub/IMAGE6.PNG)

The Raspberry Pi is a well known mini computer in its own right and full of IoT uses too. Following on from the Microserver idea, the premise of having an operating system was very appealing. Especially the idea of being able to remotely access the Hub, in which both the Microserver, Yun and Pi could provide this. The Pi while being smaller and considerably cheaper than its Microserver counterpart did lack internal hardware to boot, but for the purpose we had planned it was more than adequate. It would’t be noisy either. 

The Pi was a good middleground between the Yun and the Microserver. It didn’t have as many Operating Systems to chose from compared to the Microserver (due to its Arm architecture) but it did offer a good selection of Operating Systems in terms of networking and much more compared to the Arudino Yun. Its price was not as expensive as the Yun or the Microserver, averaging around [£25](http://www.amazon.co.uk/Raspberry-Pi-Model-Desktop-Linux/dp/B00T2U7R7I/ref=sr_1_3?ie=UTF8&qid=1459378213&sr=8-3&keywords=Raspberry+Pi+model+b). 

Internal storage could be managed using a SD card of any size meaning data logging was possible as well. With this being built in as well as an ethernet port it has many advantages over previous entries. The only piece of hardware that is lacking for what we need is an XBee connection. Although solutions are the same as the Arduino Uno, either we use a shield with XBee breakouts or we physically wire an XBee up.

However with all this, the Pi did lack the speed of other boards that didn’t require a OS to maintain. It also didn't offer built in WiFi unlike the Yun, but we were unlikely to use this anyway due to potentially changing security of a WiFi network. 

####Research Conclusion

We decided to use a Raspberry Pi (Model B+ 512MB) over other solutions. While the Arduino and FRDM K64F boards offered speed, they lacked remote accessing and long term storage and would require more adaptions to work around this. The Microserver was too large, expensive and potentially noisy. The Yun while very promising lacked internal hardware to match the Pi as well being double the price. The Pi offered a full operating system while maintaining a small size, better secure networking and remote access for updating on the network. This meant that if a bug was found in our code we could remotely update in on the hub, we would also be able to access any logged debug information from the program. 

#####Raspberry Pi (Model B+)
We decided to use a Raspberry Pi (Model B+ 512MB). The Pi offered a full operating system, better secure networking and remote access for updating on the network. This means that if a bug is found in our code while the hub is deployed in our clients house we could remotely update in on said hub. In the same way we would also be able to access any logged debug information from the program.

#####Moving forward with the Pi
The Model B+ will be supplied by the university. The operating system of choice was Raspbian Jessie Lite because it is the officially supported OS of the Pi therefore, recommended by the developers of the Pi. The Pi will have to be connected to the XBee over serial, however in order to use these ports they have to be masked by systemd to force them to be free on startup. 

Then we need to write a program capable of handling incoming AT packets from serial, interpret them and respond accordingly.

The program will be written in Python 3 as its easily available on the Pi and offers all the functionality required to create a robust networking program. We will have to modify /etc/rc.local to contain “sudo python hub.py” so that the script will start every time the Operating System starts. If the network was down, or errors occurred on transmit then the Pi will save data locally, and retry on its next attempt.

###Language of Choice: Python

*Why Python*

We have chosen Python because it was easily available on the Pi, had plenty of documentation supporting it and is a very easy language to read from another developer's standing. In terms of interfacing it with serial and the network there are plenty of libraries that exist to make this as simple and efficient as possible, we have decided to use PySerial and Requests to handle these requirements. 

*Python Libraries*

PySerial and Requests simplified any complications we may have had from writing our own initial libraries as well as having organised documentation to support them. They abstracted a lot of complicated hardware tasks (such as interrupt handling on GPIO pins) and communicating over the network. Other libraries we plan on using are those standard to Python, time for handling timing operations, random for random calculations, threading to handle multiple tasks to name a few.

###Coordinator on Network
The Hubs most important role will be that of the coordinator on the network, it is the centre point. Due to how XBees address each other, it is very easy to send data straight to coordinator using its predefined 64bit address (0x0000000000000000).  The Hub could address any node on the network and with this could determine which nodes were which and if they were still within range.

###Result of Iteration
We've successfully found the hardware to use for our Hub, the language to program in and how we plan to interface with our network. The next step is to implement AT mode, a simple system for handling messages on a network. Using the Raspberry Pi as our Hub will offer us an operating system with the added benefits that brings such as security or remote access. The Pi can easily interface with the external components we need and allow us to remotely access our low powered network if need be using SSH from outside the clients home.

####Iteration 2 - Pi with AT Mode

#####Setting up the Pi
Now that we’ve settled on an operating system, hardware and programming language we can progress to implementing a working network with the Pi. By default the Pi uses the serial ports for terminal access, for us this is of no use and we need those ports for the XBee to communicate on. In order to open the ports we had to go through Systemd which is the main configuration tool for handling debian related Linux distros. Systemd is quite new to Raspbian and because of this most tutorials offering assistance are outdated as they refer to older versions of Raspbian where the use inittab was involved. 

In order to change anything we need access to the Pi. We’ve been remotely accessing the Pi using SSH and a program called Putty, this gives us full access to the Pi without having to actually plug anything into it. Researching Systemd has shown us that you can mask services which effectively disables them entirely from starting. First we needed to find the service we were looking for.

~~~
systemctl list-units
~~~

This returned us a long list, so from this we needed to find which service was the serial port. As it transpires the Pi will always use the same serial port as it only has one - ttyAMA0. We knew what the serial port is, so we now needed to mask it.

First we decided to play safe and stop the service.

~~~
systemctl stop sys-devices-platform-soc-20201000.uart-tty-ttyAMA0.device
~~~

Then, we masked it - to stop it from starting again on a reboot.

~~~
systemctl mask sys-devices-platform-soc-20201000.uart-tty-ttyAMA0.device
~~~

![](Images/Hub/IMAGE12.PNG)

Now that serial is free we’ve got to connect the XBee to the Pi, looking at the pinout sheet for the Pi we can see which pins to interface with when compared back to our XBee. 

![](Images/Hub/IMAGE7v2.png)

The final step is to make sure we have the libraries we need for python, using a program called pip we can install these libraries. First of all pip had to be installed, which could be done using.

~~~
python -m pip install -U pip
~~~

With Pip installed, we needed to then install the libraries required for our Pi.

~~~
pip install pyserial
pip install requests
~~~

Now our Pi is ready to act as a coordinator. The next stage is programming it and making sure it knows how to act in response to our data. We need to consider the possibility of no access to the internet too, we have to ensure that data isn't lost at any point. 

#####What does the Hub need to do?
The Hub is the middleman between the webserver and the nodes on the network, it has a responsibility to ensure data from those nodes reaches their destination. We need to be able guarantee data will be logged if it cannot reach its location, or if a request can’t be completed such in the case of the clock. The Hub should wait and listen for any incoming data and once a full set of data has been received act upon it, if it’s a request from the clock - request values from the server and respond back. If it’s data from one of the sensors then that needs to be sent to the webserver. It needs to be able to distinguish between a sensor and a clock otherwise it’ll send data to the wrong nodes or request values from the webserver for the wrong reasons. 

![](Images/Hub/IMAGE7.1.PNG)

######Basic Structure
The Pi will run a thread that continuously waits on serial input, once received it will take as much as it can in before analysing what it’s received. Upon analysing it will decide whether the data is a request from the clock or sensor data, if sensor data it will attempt to transmit it to the webserver, if a request it will request the last 24 hours of average sound values from the web server. If a clock request is made and the web server does respond then the hub expects a format of 24 integer values in an array, when these values are obtained it forwards them to the clock. 

######Data backup
In case the connection between the hub and webserver fails, we need to ensure data backups. In the case that the network fails the Pi will write all of its currently available sensor data to a local file, it’ll re attempt to transmit data the next time it receives another set of sensor data. If that fails, then the cycle continues - save data and try again next time. The hub does try a total of 5 times before giving up and saving to a file, just in case there was a particular error that occurred.

######Distinguishing traffic
The hub needs to be able to tell which node is transmitting which data to it, how does it know whether the data it's received is that of a clock making a request or a sensor sending data? The clock sends data in a format of “R:!”. This is unique, it never appears in any of the sensor data and so when the hub receives any data it will scan for this particular set of characters. If received, it will know that this is a request and not sensor data. Otherwise it will assume all incoming data is from the sensor and forward it to the webserver. 

###Result of Iteration
Although we now have a functioning Hub that can transmit and receive between a clock and sensor, it can't handle data coming in from multiple sensors. It also can't fragment large payloads which is quite a concern but we have managed to bypass this by using the internal timers of the XBee hardware to our advantage. However that doesn't make that a good solution and a long term fix will be a priority in the next iteration.

Overall the system can function, but if any major strain occurs then the system will suffer. The Hub can handle if the network goes down and will backup data when needed. It can request values for the clock and transmit sensor data but only from one sensor on the network, anymore and data will become malformed.

Our next iteration will focus on fixing packet fragmentation, handling multiple sensors and hopefully error recovery.

####Iteration 3 - Pi with API Mode 
Now that we’ve changed from AT mode to API mode (see [Networking Iteration 4](#network_i4)), not much needs to change but at the same time the properties of the Hub have greatly expanded. The API has been designed so that more functionality could be provided without requiring an excess amount of modification to existing code. 

The basic idea being, we only need to change one line:

~~~python
serial.write(“Hello world!”);
# becomes …
response = xbee.sendMessage("sensor1", "Hello world!")
~~~

Now although we’ve added an extra parameter, that is purely beneficial. The extra parameter allows us to address individual nodes on the network, if we want to address every node we specify ‘broadcast’ as the node on the network.

~~~python
# Will message every node on the network with “Hello world!”
response = xbee.sendMessage(“broadcast”, “Hello world!”)
~~~

The nicknames for nodes helps in multiple ways, not only does it allow us to forward this data to be used in identifying different sensors, clocks etc but we can use it to show the client a name that is more legible than a series of hexadecimal values. 

#####Hub Python Library

For information regarding the process behind designing and researching the API required for the Hub, please see [Networking, Iteration 4.](#networking_i4).

The Hub is utilising a library written to handle the API mode of the XBee, the library has many purposes that help make the network as robust as possible. Using these features we’ve been able to make our coordinator incredibly robust as handling large payloads, transmission errors, node discovery and error recovery.

Below is a flowchart diagram demonstrating the new processing behind the Hub.

![10](Images/Hub/IMAGE9.1.png)

######Initialising the Library

Initialising the library consists of a standard object creation in Python with one parameter - the serial port.

~~~python
# Create API, pass the serial port
xbee = XbeeAPI("/dev/ttyAMA0")
~~~

######Receiving messages

Using the library, you can retrieve stored packets using these methods. 

~~~python
# Get most recent message, returns 
# None if no messages available
message = xbee.getMostRecentMessage()
if not message = None:
	print message.getPayloadAsString()

# Get oldest message, returns 
# None if no messages available
message = xbee.getOldestMessage()
if not message = None:
	print message.getPayloadAsString()

# Get all messages, returns 
# None if no messages available
messages = xbee.getMessages()
if not messages = None:
	for packet in messages:
		print message.getPayloadAsString()
~~~

######Node Discovery and Heartbeats

Being able to determine what sensors already exist on a network offers us much more functionality. We’re are able to ping nodes on the network and determine their network status.

On startup the program running on the hub will  load from memory any nodes it has previously found, after this it’ll try to find any new nodes on the network and save them. This way, it can determine if a node was missing (Maybe a sensor was moved) and could report this back to the client. The hub will periodically send heartbeats across the network to see any changes on the nodes. If a change is detected (such as a node disappearing) it will report this back to the server, this way we can display this information on the website that a sensor was out of range or had run out for battery. 

The Hub utilises the ability to periodically send heartbeats to nodes on the network, this ensures that nodes are up to date - it also falls under node discovery as it will use heartbeats to add new nodes to the network. Once the Hub detects a node its unfamiliar with, it will request its ‘nickname’ for assigning it in its map. The format being:

~~~python
# Send node a HB request asking for it's nickname
response = self.sendMessage(node, "HB#:")
#  Wait for node to respond
time.sleep(.25) 
 # Failed to contact the node
if not response == 0:
	return
#  Add the node to the map along with its 64bit address
# …
self.destinationNodes[name] = newNode
~~~

If the Hub fails to contact a new node, it will reattempt the next time the node retransmits to it. 

With node discovery it simplifies sending messages from the coordinator to the sensors, instead of requiring individuals addresses, you can pass names of nodes into the “sendMessage” function and it will conclude the address based on this from a map structure storing a key to a value. The key being the nickname pointing to the 64bit address, this could be furthered if we implemented the 16bit addresses, which would avoid an address lookup.

~~~python
# Send message with API, sensor1 lets the API know which 64bit
# address we’re looking for, returns successful or not
response = xbee.sendMessage("sensor1", "Hello World")
~~~

######Packet Transmission Status
The Hub stores a single transmit status packet at a time, due to the structure of the library it will only ever need one as it will always check against its transmissions before attempting to transmit again. 

Previously we’ve been unable to determine whether a packet was received or not without physically checking the receiving node. With the new library we’re able to get a response back from our ‘sendMessage’ function, here is a list of the possible response codes:

    0 = successfully transmitted all frames
    1 = payload too big, more than 255 frames required
    2 = Invalid node, does not exist on network
    3 = Failed to transmit data to device, could not reach node

So the Hub can now respond as needed.

~~~python
sensor = “sensor1”
response = xbee.sendMessage(sensor, "Hello World")
if not response == 0:
	print “Failed to reach”,sensor,”!”
~~~

The output of this code if under the circumstances the XBee failed to transmit would be “Failed to reach sensor1!”. 

With this new feature we can report back to the webserver if a sensor or clock is out of range, or if for any other reason we can’t contact them. Similar features are offered for the sensor as well, meaning that they can also determine whether they are out of range or if the Hubs XBee has failed. 

######Packet Fragmentation 
Previously we’ve been unable to transmit large payloads without potentially malforming data, with the new library we can successfully reconstruct packets based on their frame IDs and source addresses. 

When the Hub needs to transmit a message above the MTU for RF data it will begin fragmentation of that message. It calculates how many frames are required for this packet and breaks it down into separate frames for transmission. It prefixes each frame with its ID and upon the final frame suffixes it with the unique termination character ‘!’, which will not appear in normal transmission.  

The Hub will transmit a frame and wait for an acknowledgement from a status packet before transmitting the next packet, this guarantees that frames cannot arrive out of sync. If it fails to transmit multiple times on the same frame it will return an error code.

![](Images/Hub/IMAGE 11.png)

######Packet Assembly
The Hub stores a list of messages it has received and arranges them based on current frame ID as well as source, it can determine whether a message has terminated upon final frame and return the source of the transmission. When a frame is received the Hub will check all stored messages, if it finds a packet with the same source address that hasn’t terminated and shares the same current frame ID it will append the RF data to the packet contents. 

Due to the structure of the library it is impossible to send frames out of sync, as each frame is checked to ensure it was received before transmitting the next frame. If a packet fails to be terminated after a certain time window of the last frame received it will be dropped. 

##### Results of Iteration
The Hub now using its new API Mode library can offer huge benefits to the network and the system as a whole. As the coordinator it controls effectively all data coming in and out. It offers node discovery, error correction and recovery, heartbeats and feedback on any disappearing nodes. With this addition the hub can now report back useful information to the webserver for analysis such as a node disappearing from the network due to battery failure or being out of range. 






## Server
[Back to contents.](#contents)


###Description

As there are multiple users wishing to see data coming from multiple sources we needed some sort of central service that collects the information and displays it in an easy way for the users to see. 

The server acts as an API where all data is sent to and requested from. This allows us to add different visualisations very easily without the data being coupled with the interface. We then store the data in the database and it can be requested by other visualisations by making calls to the API endpoints.

There is a key which must be sent with each request to ensure that it is a device that is allowed to access that data. The request should be made over HTTPS to avoid exposing the key. The key is hardcoded into the server but can be changed by modifying the code if needed.

To get a development server running first install a copy of [Laravel homested](https://laravel.com/docs/5.2/homestead). Run through the setup guide and clone the repository into the specified directory. After this you can run need to insure all dependancies are installed by running `composer install && bower install`. After this you can run `homestead up` and visit the address you specified in the homestead setup.

As the server is using Laravel you can easily set up the databases without manually running SQL queries.

In the root directory of the app you can run the following command to create all of the database tables.

`php artisan migrate`

This creates tables based on a model definition, an example definition can be seen below: 

```php
class CreateUsersTable extends Migration
{
    /**
     * Run the migrations.
     *
     * @return void
     */
    public function up()
    {
        Schema::create('users', function (Blueprint $table) {
            $table->increments('id');
            $table->string('name');
            $table->string('email')->unique();
            $table->string('password');
            $table->string('phone');
            $table->rememberToken();
            $table->timestamps();
        });
    }

    /**
     * Reverse the migrations.
     *
     * @return void
     */
    public function down()
    {
        Schema::drop('users');
    }
}

```
This uses the built in 'schema builder' to turn the php code into sql which is then run on the database.


###API Spec
The API responds to HTTP requests which conform to status codes set out in [RFC2616](https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html). 

Key middleware is present throughout the API and it's responses are the following:

| Status | Response|
|--------|---------|
|403		|`{"error":{"text": The key you provided was incorrect}}`|
|401| `{"error":{"text": You did not provide a key}}`|

The endpoints are as follows:
####Readings
*Request*

|  Method | URL  |  
|---		|---|
|  POST | /api/readings  |

| Type | Params | Value |
|------|--------|-------|
| GET	|	key		| string|
|POST| 	data	| [data object](#data_object)|


*Response*

| Status | Response|
|--------|---------|
|200		|`{status:success}`|
|400| `{"error":{"text": PDO Error}}`|

<a name'data+object'></a>
####Data object
The data object is a csv string which contains both a timestamp and a microphone reading. It also contains the battery reading of the sensor. The csv should be formatted as shown below:

```
'id',id
timestamp,reading
'batt', battery_percentage
```
example with values:

```
id, 2
14073748529, 3547
14073748530, 3047
14073748531, 3538
batt, battery_percentage
```

Please ensure all lines are separated with \n\r

####Get
Returns 24 hours worth of data averaged over one hour periods
*Request*

|  Method | URL  |  
|---		|---|
|  get | /api/get  |

| Type | Params | Value |
|------|--------|-------|
| GET	|	key		| string|


*Response*

| Status | Response|
|--------|---------|
|200		|`{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,12,22,23,24}`|
|400| `{"error":{"text": Database Error}}`

####Get Hourly

Returns all data averages over hourly periods
*Request*

|  Method | URL  |  
|---		|---|
|  get | /api/get/hourly  |

| Type | Params | Value |
|------|--------|-------|
| GET	|	key		| string|


*Response*

| Status | Response|
|--------|---------|
|200		|`{0:{start:183975627; end:183975687; avg: 12.4;}1:...}`|
|400| `{"error":{"text": Database Error}}`


####Get Hourly In range
Returns all data recorded between 2 given timestamps averaged over an hourly period.
*Request*

|  Method | URL  |  
|---		|---|
|  get | /api/get/hourly/{start}/{end}  |

| Type | Params | Value |
|------|--------|-------|
| GET	|	key		| string|
| URI  | start	| timestamp (string)|
| URI  |end		|timestamp (string)|


*Response*

| Status | Response|
|--------|---------|
|200		|`{0:{start:183975627; end:183975687; avg: 12.4;}1:... }`|
|400| `{"error":{"text": Database Error}}`

#### MVC
The server is built using the MVC layout which allows for decoupling of elements and helps to make the structure understandable. As seen in the diagram below, data mainly flows one way.

![](Images/visualisation/mvc.png)

In the server, the views are very minimal as it is mainly just displaying json data. While the views aren't the same as in most applications, they still work in the same way.

The server has three main models:

- Device: This represents a physical device and stores things such as battery level and name
- Reading: This is a data reading and links to a Device. It contains the raw data and timestamps
- User: This stores the users details such as email, phone number and password

These are all called by the controllers who then pass the data straight onto the views to display it.

###Previous Work

####Iteration 1 - Initial Work
***At this point we had thought visualisation and back end server were the same thing. This was later changed but for documentation purposes we have considered the initial itererations of this as part of the server***

As there are multiple users wishing to see data coming from multiple sources there needs to be some kind of central service that collects the information and displays it in an easy way for the users to see. 

One option for this is to use the hubs themselves as servers. This means that there would be no extra hardware required and everything to run the system would be in the user’s home. This poses several problems, one of which is that the hardware is in the user’s home so if something were to go wrong with the software and it were to crash the device would require a restart on the user’s end which would mean either asking them or going into their house. During this time there would be no results being received by the server and  you would not be able to view any previous results. It also means asking the hub device to do multiple things which means there are more things that could go wrong.

Another option is to have this service run on the university’s servers, this approach also has several pros and cons. One of the benefits is that all the infrastructure and software to run a server and databases are in place and we would simply have to ask for one to be set up. Another good thing about using the university’s infrastructure is that we have greater control and there are safeguards already in place for if things go wrong. One downside to this method is that currently, to access the servers, you need to be connected to the University’s network by being on campus or over VPN. This could be a big problem as the the devices will not be located on campus and ideally would not be connected to the VPN as it can become complex and is yet another thing that could go wrong. One way around this that has been suggested is to use the server that The Shed has. We have been told by Dan Knox that it would be possible to open up some of the ports on that server to allow connections from outside the University’s network.

Overall the option of having a web server in the university would be the best option as it is already provided as a service and is maintained so there would be less maintenance for this project. This decision is relies on being able to open up the sheds servers for outside connections. If it turns out this is not possible then we will have to revisit the hub/server idea and possibly think of some other options.

#####What we need
The basic idea of the web end of this project is to provide a place to store large amounts of data over a large period of time and have some sort of mechanism where the user can see those results in a meaningful way. The devices/hubs also need to be able to connect to this service so they can send results directly to it. The user also needs to be made aware of device status such as whether the battery is running low or the device has crashed and needs restarting. This means that the main requirements of the web service are as follows:

- Provide an interface the hubs can easily talk to
- Have the ability to store large amounts of data, around 2.5 million records per year (365 days * 24 hours * 60 minutes * 5 devices which is a reading every minute)
- Show the information that has been collected in a meaningful way that makes sense to the user.
- Provide users with notifications, in the form of emails, regarding the status of devices
- Be easily maintainable as it may need to be picked up by other developers in the future
- Process data. The data sent from the hub will be the raw data from the sensors. The web service needs to turn these into meaningful values and store those

#####Technologies
######Database Comparison
One thing that the web end will definitely need is a database to store all of the data that it is being sent from the sensors. There are many different types of database that are available but we narrowed it down to just three based on what we had experience with and were comfortable using in a short space of time. These three are mongodb, PostgreSQL and MySQL.

*MongoDB*

MongoDB is a NoSQL database that has not defined structure. You can add and remove fields in each record as you see fit. It also favours a high insert rate over a high read rate which is what our application would be doing as a majority of the work would be coming from the sensors rather than people wanting to view the data. The variable structure of this database would make it easier to add sensors going forwards however even though there is no explicit structure in the database itself it would just move the structure over to the web app instead. This means that the web app know what data is stored where and where relations between it should be. This means more time has to be spent on the web app to make sure the structure makes sense. There would still need to be many changes made on the app side if a new sensor type was introduced. Also, as it is a relatively new technology the documentation is not ideal and it is harder to pick up than a standard relational database if you have never worked with NoSQL before. 

*PostgreSQL*

Postgres is a fully features RDBMS that is widely used and growing in popularity. There is a large community around Postgres meaning there are a lot of blog and knowledgebase style articles that provide a lot of support and make development easier. One of the disadvantages is that it provides a lot of functionality that will be unused for simple reads and writes which is what this application will be doing. A benefit of postgres is that it is provided by the university and can be set up by someone outside the project.

*MySQL*

MySQL is another RDBMS and is the most popular one. One advantage of MySQL is that it is very easy to pick up and get started with as it only provides a limited set of features. This could be a downside but it provides everything that this application would need. It is slightly faster than Postgres and there is a lot of support on the internet for it. It is also something that can be set up and maintained by the university.


All of these databases can handle large amounts of data >20 million records which means they are all suitable for the volume of data that will be necessary for this project.

######Language
Because the application needs to run on a server, it needs to be written in a language that a server can be easily set up to understand. PHP was chosen for this as it was a language that everyone in the group is familiar with and has had previous experience using. PHP also fits all of the requirements and is very well documented. 

######MVC Frameworks
Frameworks are tools that are designed to make the development of applications easier by providing a set structure and providing some commonly used features such as authentication. This means that there is less time spent re inventing the wheel and more time can be spent on application specific code. The decision to use a framework was made as one of the key requirements is maintainability. Frameworks provide a lot of documentation to make understanding them very easy. To create something that was tested and documented to the level of an existing framework would take a long time and we felt we could better spend this time in other areas such as UI development. 

Several frameworks were looked at: 

*Laravel*

Laravel is an open source MVC PHP framework. Laravel provides a lot of features out of the box such as an ORM (used for creating and maintaining relationships in databases), a templating engine, database migrations (used to control the structure of your database) and range of other packages handling things such as payment and social logins that can be easily implemented. A benefit of this framework is that it could be very easily extended to add complex features with very little effort and development time. Another good thing about Laravel is that the documentation is well maintained and is regularly updated.

One downside is that it does a lot of things that are simply not necessary for this project. These unnecessary features increase the learning curve of this framework even though it can make things easier in the long run.


*Slim*

Slim is a simple PHP framework that is often used for simple web apps and apis.  It doesn’t provide some of the more advanced features that laravel does but it does give you things such as routing, middleware (for things such as authentication), and easy http manipulation methods.

One downside to Slim is that it is not hugely popular so there are less tutorials and help threads for it but it does have it’s own set of well maintained documentation which means this isn’t a huge issue. 

A benefit is that it doesn’t try to do everything for you, just a core set of things which means it’s easy to pick up and requires very little time to understand. This is key for the maintainability part of the requirements. It also gives you the ability to create simple views to display data without having to learn another templating language, it does it all in PHP.  

*CodeIgniter*

CodeIgniter is another widely used MVC framework that provides a lot of features straight out of the box. As with Laravel, the downside to this is that there is a lot to learn and it’s not instantly obvious what is going on. One of it’s main advantages is its speed compared to other large frameworks however this is less of an advantage when comparing it to lighter frameworks such as slim. It also has a very active community which means documentation is readily available and kept updated.

*Conclusion*

After looking at the different frameworks that were available to us we decided that the best one to use was Slim due to its simple nature. It meant that we could all pick it up fast and get working quickly while still having good documentation that would be easy for anyone else to pick up.

######Version Control
Version control was necessary for this projects as it provided a log of what had been done which we could revert back to if anything went wrong. It also allowed us all to view the code so it could be looked over to check what was being written and committed was sane.We decided to use git for version control as it is an industry standard and we all had some experience of using it. We are hosting the remote repository on Github as it provides an easy to use interface and also allows transfer of repository ownership should someone else wish to take the project on in future. 

##### Outcome of Iteration
We produced a system that would take data it received from requests and store it in a database. This system ran in a local development environment so was no accessible outside of the computer it was running on. It also showed pages that displayed basic data such as a table of timestamps and readings. This was written using the slim framework


#### Iteration 2 - Splitting Front and Back End

##### Issues with previous iteration

During the previous iteration we discussed putting the application on the university servers however this hasn't been set up so we have used a temporary sever that we are hosting ourselves using digital ocean. The server is using the same technologies as previously mentioned and is running on ubuntu 14.04. 

Another issue we became aware of after the implementation of the first iteration was that the data and the views were very tightly coupled together. We then decided that it would benefit the project to split the front end and back end up into two separate parts. The server would act as an API and the front end would be used to display the data. This would allow us to add the 'clock', which we weren't aware of at the beginning, to the system without any major issues.


#####Outcome of iteration
We worked to split the front end code from the API and the front end so that they could work independently without duplication of code. We realised some kind of security was also needed so we added key based authentication to provide a basic level of security. 

#### Iteration 3 - Switching Frameworks

##### Issues with previous iteration
After we split the front and backend we realised we were going to be using slim to server the backend and Laravel to serve the front end. This would mean using someone who was picking this up would have to learn multiple frameworks.

##### Result of iteration
We decided to change the backend the also use laravel to make the system easier to pick up. Most of our code was easily portable as they use the same language and roughly the same MVC layout. 

#### Iteration 4
#####Issues with previous iteration
While working on the visualisation we discovered that filtering on the client side was not an option as it took too long to load. This meant that we had to do averaging and filtering on the server side. This was due to loading large amounts of raw data. For example to visualise 1 month worth of dummy data it was having to load ~600,000 (60\*24\*7*4) readings which took a considerable amount of time. It then had to group this data and calculate averages based on this which, again, increased page load time. The total load time of a page loading this amount of data was ~6 seconds

The sensor was also struggling to create json arrays due to memory limitations. This meant that the format the server was expecting was often not the format it received. 

#####Outcome of iteration
We changed the format of data being sent to and from the sensor from JSON to CSV. This meant there was less overhead in generating the data on the sensors and also less overhead when receiving data.

We moved a majority of the data processing over to the server rather than the client. This was mainly done by using SQL queries to group and average the data. As this is what the database engine is designed for it was able to considerably speed up this process.

Server code can be found [here](visualisation/web/)

More information on the framework used and where the main files are located can be found at the [laravel website](https://laravel.com/docs/master)

<a name="visualisation"></a>
## Visualisation 

[Back to contents](#contents)

### Description

The web visualisation aspect of the system allows users to both get a quick overview of that data that has been recorded but also dive into the data for a much more detailed view of it. This was an important part of the system as it had to make noise levels clear while also providing the raw data that would be used to back that up.

We created a user account system with the aim of allowing users to receive next notifications for each of the devices. This system lets users register, sign ing, update their information and view the data. Another reason we did this was initially, the residents were concerned that the bars may look at this data in some way and use it to try and benefit them. The account system allows only certain people to view the data while also allowing the residents to let people sign up such as council members. The accounts system used Laravel, which was what was also being used for the API.

Another part of the web visualisation was a sensor status section. From here the residents could check on things such as battery status and last reading so they could be happy it was working.

![](images/visualisation/status.png)
The finished web visualisation used D3.js to create a number of different ways of displaying the data. The main three it uses are the clock, graph, and compare.

####Clock interface
The clock is intended to bridge the gap between the physical parts of the system and the digital parts. It provides a consistent representation of data between the two in an effort to make the user more comfortable. An image of the physical and digital clocks can be seen below:


![](images/visualisation/clock.jpeg)
![](images/visualisation/clocks.png)

The clock view is meant as an overview for the entire system, at a glance you can see what has been chosen. The clock allows you to then click through to a more detailed graph.

####Graph
The graph shows a more detailed view of the day allowing you to see what numerical levels the noise reached. It also includes a list of the raw data which consists of timestamps, the raw reading, and decibels. 

![](images/visualisation/graph.png)
![](images/visualisation/data.png)

The graph combined with the raw data output shows the noise level in an easy to understand way while displaying important information. 

####Compare
The compare section of the website was created as a way of viewing multiple days worth of data on one page. We previously tried this with graphs but found that it was messy and didn't get the point across. To solve this we created a view that which, like the clock, displayed the data using colours based on the sound level. It displayed hours across the x axis and days on the y axis.

![](images/visualisation/compare.png)

As you can see in the image above, on Saturday and Wednesday evening it was louder than other evenings as they have a yellow colour rather than green.

####Feedback 
We received feedback throughout the process on our ideas from the client, however this feedback was very informal and was mainly focused on how it looked rather than the actual functionality. Near the end of the project we tried to get some more formal feedback from the client but unfortunately we could not meet due to various other arrangements. Due to this we had to test it on people we knew. 

We set out some tasks for the testers. One of us sat down and asked the questions while there was another person chosen as an observer. They took notes on anything important. Below is a summary of those

*Task 1 - log in and out*
We gave the testers some login details and asked them to log in and then back out again.

All of our testers seemed fine with logging in, however, some struggled with logging out as it is in a dropdown below the username. 

*Task 2 - Explain what the clocks mean*
We showed the testers the clock page and asked them to explain what it represented.

Everyone seemed to understand the general idea behind it although some were confused about whether it was 12 or 24 hours. 

*Task 3 - Show detailed data about specific day*
We asked the testers to view details about a specific date.

This proved quite difficult for some people, initially they were looking around the top menu for another page as people didn't realise you could click on the clocks. They eventually worked it out but it took some time. Most people were fine.

*Task 4 - Explain the compare page*
We asked the testers to explain what the graph on the compare page meant.

After looking at the clock, everyone seemed to understand this fairly well and there were no major issues.


We'd like to improve on this feedback in future iterations of the site however there is currently not enough time in the project.

###Previous Work
#### Iteration 1 - Initial Work
We wanted something that we could use to show the data we had gathered from the sensors in an easy way. We began by using the 5-sketches-or-else method. This started with us each sketching 5 ways we could display this data individually. After we had done that we came together to present our ideas to each other and discussed what we liked about each one. We then worked together to merge the best elements from all of our designs into new designs. After this we continued to improve these designs until we got to a point we were happy with. You can view all of our initial sketches and improvements [here](sketches/)

The main things that came out of this process were the clock, line graphs and a calendar style view. 

After this we decided to start to consider implementing them. We found that some members of the group had experience graphing data and made the most of that experience.We decided that we would generate the graphs and charts using JavaScript rather than generating them on the server as it would move load from the server to the client and also make it much easier to create as we were familiar with JavaScript. We also decided to use a library as it would allow us to quickly get something out there as we wouldn't have to create it form scratch. Based on the sketches and ideas we made and came up with we came up with the simple library comparison chart seen below.

<table>
	<tr>
		<th>Name</th>
		<th>Difficulty</th>
		<th>Line Graphs?</th>
		<th>Calendar Style?</th>
		<th>Clock/circular Style?</th>
		<th>Well documented?</th>
	</tr>
	<tr>
		<td>Chart.js</td>
		<td>Easy</td>
		<td>Yes</td>
		<td>No</td>
		<td>Yes</td>
		<td>No</td>
	</tr>
	<tr>
		<td>Highcharts</td>
		<td>Medium</td>
		<td>Yes</td>
		<td>No</td>
		<td>Yes</td>
		<td>Yes</td>
	</tr>
	<tr>
		<td>D3.js</td>
		<td>Medium</td>
		<td>Yes</td>
		<td>Yes</td>
		<td>Yes</td>
		<td>Yes</td>
	</tr>
	<tr>
		<td>Flot</td>
		<td>Easy</td>
		<td>Yes</td>
		<td>No</td>
		<td>No</td>
		<td>Yes</td>
	</tr>
	<tr>
		<td>jscharts</td>
		<td>Easy</td>
		<td>Yes</td>
		<td>No</td>
		<td>No</td>
		<td>No</td>
	</tr>
</table>

After looking at this we decided to go with D3. It would give us the flexibility to add more complex charts such as one in the style of a calendar as well as being relatively easy to use, well known and well documented.


##### Result of iteration
We created a simple page that would display a graph of a single day along with all of it's points. This is very simplistic and is just to ensure that we can get our data in the correct format.

####Iteration 2 - Displaying Data over multiple days
During this iteration we wanted to display multiple days worth of data on the same graph.

#####Issues with previous iteration
Processing the data is taking large amounts of time on every page refresh

#####Result of iteration
We now have a graph which can display multiple days worth of data. To select these days there is a calendar which you can click on, clicking on the same date again will then deselect it. The graph updates without needing to refresh the page. It is also much quicker than before as we moved the processing of the data from the front end to the API.  More on this can be found in the [Server section](#server)


####Iteration 3 - Clock visualisation
We wanted to make a bridge between the clock and the website so we decided to create a digital version of the clock that could be displayed on the website. 

#####Issues with previous iteration
The previous iteration worked fine technically, however there were some issues with the way the data was displayed. Any more than 3 lines was complicated and meant it's difficult to see what data you are looking. We will improve on this in a future iteration.

#####Result of iteration
We created a visualisation that looks similar to the clock as you can see below:

![](images/visualisation/clock1.png)

This was created from the test data we received over christmas.

The clock is built on top of D3 using an extension called Circos.js. Below is the code that is used to create these circles

```javascript
//Get the data from the API
d3.json('http://orange.app/api/get/hourly')
		.get(function(error, json){

			//Loop over each set of data received and convert it into a clock
			charts[0].forEach(function(x){
				data = json;
				parent = d3.select("#"+x.id)[0][0].parentNode;
				width = parent.offsetWidth - 50;
				height = parent.offsetHeight ;
				var circos = new circosJS({
					container: "#"+ x.id,
					width: width,
					height: height,
				});


				layout_data = []
				
				//Create the layout data used by Circos.js by creating array of 
				//divs with ids 0 - 23 which represent hours
				for (i = 0; i < 23; i++)
				{
					if(i < 10)
					{
						layout_data.push({"len":1, "id":'0'+i+'hour'});
					}
					else
					{
						layout_data.push({"len":1, "id":i+'hour'});
					}

				}

				var clock_data =[]

				//Turn the data received from the api into the correct format and 
				//add it to an array containing all the data from our clock
				json.forEach( function(d){
					if(ymd(new Date(d.start)) == ymd(new Date(x.id.substring(1)))) {
						var date = new Date(d.start)
						var hour = date;
						hour.setHours(hour.getHours() + 1);
						clock_data.push([h(date) + "hour", 0, 1, (d.avg+15)])
						console.log(+d.avg+10);
					}
				})

				//Construct the circos object
				circos
					.layout(
						{
							cornerRadius: 3,
							innerRadius: width/4+25,
							outerRadius: width/4+30,
							ticks: {display: false},
							labels: {
								position: 'center',
								display: true,
								size: 14,
								color: '#000',
								radialOffset: 15,
							}
						},
						layout_data
					)
					//Show what colours you want the sections to be
					//based on colorBrewer
					.heatmap('temperatures', {

						innerRadius: width/4,
						outerRadius: width/4+25,
						min: '23',
						max: '0',
						colorPalette: 'RdYlGn',
					}, clock_data)
					.render();
			})
		})
```
####Iteration 4 - Fixing the line graph
The multiple line graph that we created in iteration 2 was confusing hard to understand. We planned to fix this in this iteration.

#####Issues with previous iteration
None

#####Result of this iteration
We changed the way data was graphed so you could no longer select multiple dates. This made the graph much easier to read but we would need another way to compare data. We began to look at ways of comparing however there are no obvious ways and our sketches don't really help with that.

####Iteration 5 - Fixing data comparison
As mentioned in the previous iteration, we removed the feature from our line graph that would allow us to compare multiple days. This comparison is an important part of the website as without it the results can be hard to understand. 

#####Issues with previous iteration
None

##### Result of this iteration
After looking at several sources we decided that we liked how GitHub show
how often someone commits. You can see an image of one of those graphs below:

![](images/visualisation/github.png)

As we are working with data where time is important we cannot simply show the day so we changed it to include hours on the x axis and days on the y axis. You can see the graph we created below:

![](images/visualisation/compare2.png)

####Iteration 6 - Turning graphs into a website

Up until this point all of the graphs had been standalone, pulling the data from the API but all separate. We needed to create a website where all of these could be viewable and make sense together.

#####Issues with previous iteration
There were some issues with aligning the graph labels however this will be left until a future iteration.

#####Result of this iteration
We created a website that was built in Laravel and using bootstrap to improve the interface. We created a user accounts system that would allow access control in the future. The code for the website can be found [here](visualisation/web/resources/assets/js/)

<style>
	.todo{ color:red }
</style>
[toc]

##Manufacture / Casing [cont.](#contents)

###Initial Research

The project required us to have hardware in the real world meaning that it has to be able to survive in the environment that it is deployed in. We couldn't simply deploy the electronics because they would get damaged. Therefor we have to house all of our pieces of hardware in some form of casing.

There are a few ways to make cases, they include making a custom 3D model of the object and sending it to companies to either be 3D printed, a process where plastic is melted and printed in layers to form the intended 3 dimensional object. Alternatively, injection moulding, a process where a mould is made of the object intended to be created then melted plastic fills the negative space in the mould leaving the object when it has hardened. 

Injection moulding creates a rigid plastic shell when hardened as the piece comes from liquid plastic that hardens into one piece whereas 3D printed pieces can be more brittle as the plastic dries on when each layer is added meaning it can leave pores on the final product.

There are companies that offer custom 3D printing and injection moulding such as [Shapeways](http://www.shapeways.com/) (3D printing) and [Protolabs](http://www.protolabs.co.uk/) (3D printing and injection moulding). The main issues with these places is the cost and time it would take to get an object back. We would have to design the object we want created, each piece is individually quoted, it would be created and we would have to wait for it to arrive. In the case of injection moulding Protolabs have a minimum of 25 parts from one mould, meaning it would be more expensive than 3D printing. If there was a mistake in a 3D model we would have wasted time and money and still have to print another one.

Although, The Shed has specific software for 3D modelling and a 3D printer that we can use for prototyping and final printing of our cases. It is possible to quickly print a prototype and test a design multiple times and print a final when we are happy with it. 

####Conclusion

We decided to use the Sheds facilities to create our cases. It will be much more cost efficient as we do not have to pay for each individual print and the turn around of each piece is much quick as we do not have to wait for it to be posted.

###Hub Case

####Description

The Hub case is 3D printed. It was designed to hold the Raspberry Pi in place through the use of screws and nuts. It is also designed in a manner that made it easy to open incase there were any problems while the hub was deployed. The case comes in two parts, the lid and the base.

The case is meant to be easily modifiable. So that if there are any changes that need to be made it is easy enough to change the 3D model and print off a new one that fits the purpose better.

*Base*

The base is made to be 2mm thick on the longer sides for strength and rigidity. It was made to be slightly bigger than the Raspberry Pi model B+ (5mm in every direction but the one with the ethernet connection). It has holes in the bottom big enough for 2.5mm fixings to fit through both the case and the board using nuts to hold it in place so that the board is secure inside.

There are two grooves on the longer sides of the base, inside the base, that allow the lid to click into place.

PICTURE OF THE BASE 

*Lid* 

The lid is designed to be reversible for ease of use. The longer sides of the lid has sections that sit inside the base. These sections are as long as the inside of the base so that it does not slide around when fitted. These extruded sections have further extruded lines that click into the grooves on the base. To remove the lid squeeze the sides and pull off. It was designed in this way for ease of access.

PICTURE OF THE LID

####Previous Work

#####Iteration 1 - Initial Hub Base.

The FRDM K64F board had no technical specifications that could be found meaning we did not have the measurements for it. Therefore it had to be measured by eye using rulers and electronic calipers. 

INSERT INITIAL SKETCH

As this is the fist time we would be 3D printing anything we had no reference for how strong the result would be. We decided to measure the thickness of the calipers box as that seemed to be quite a strong (2.5mm).

We decided to model the bottom half of the case first. This was done because it was the smaller of the two pieces of the case, we could reprint a new one quickly if there was anything wrong with this one. We wanted the board to be secure in the case so the we designed it in a way that the base, lid and board had holes that lined up so 3mm fixings could be screwed through all three and secured together.

INSERT PICTURE OF PRINTED CASE

######Outcomes of iteration

We 3D printed our first object, learnt how to use the software properly.

#####Iteration 2 - 

###Sensor Case
####Description
####Previous Work
#####Iteration 1

###Clock Case
####Description
####Previous Work
#####Iteration 1
##Clock
###Description

![Final Clock](Images/Clock%20Images/clock_final.jpg)


The clock is our form of ambient data visualisation, and is designed to engage and notify the user. It is the physical counterpart to our front end visualisation, and uses a simplified system of visual data to represent noise levels. It is designed to complement the more technical premise of the web server. 

It uses a series of 24 LEDs arranged in a ring, which, every hour are lit to a colour corresponding to the average sound levels for that hour. This is a lightweight way of giving the user a quick glance at the sound levels, and allows for comparisons and references to be made across a 24 hour period. 

The light display is handled by an Adafruit NeoPixel Ring 24 x WS2812, which is interfaced through an Adafruit Flora used as the microcontroller. The interaction of the clock with the rest of the system involves an XBee device configured as a router. Resultantly the clock can receive values from the hub, and stay up to date in terms of the data it is presenting.  
Every hour, the clock communicates with the hub to receive the average sound level across that hour. The clock turns on a light for that hour, depending upon the corresponding sound level. Over time, this builds a picture of the day’s sound activity, mirroring the data visualisation on the web server as a physical model. 

Components: Adafruit NeoPixel Ring 24 x WS2812, Adafruit Flora, Communication/XBee (Lilypad XBee), Case. 

####Adafruit NeoPixel Ring 24 x WS2812

The central hardware component of the clock is the Adafruit NeoPixel Ring. One of several NeoPixel models provided by Adafruit, the 24 x WS2812 is a collection of 24 addressable LEDs arranged in a ring. The LEDs were directly addressed by the Arduino code written for the Adafruit Flora, allowing direct control over the display, and timings that each LEDs came on. 

####Adafruit Flora

Lightweight and circular in design, the Adafruit Flora was an effective choice for maintaining the streamlined structure of the clock’s components. Since it is typically used in wearable tech, the Flora is a microcontroller which is smaller than the NeoPixel Ring. This meant the overall size of the clock could be kept compact, allowing more focus to be on the visual display of the NeoPixel Ring. Moreover, it offered the functionality we needed of similar microcontrollers such as the Arduino Uno, but stripped away the unnecessary features. Since the clock device could be powered by mains, the power consumption was not too much of an issue. 

####Communication/XBee (Lilypad XBee)

As with the other devices in the network, the clock communicated using the Zigbee protocol; using an XBee RF module attached to its microcontroller. It was configured as a router on the network, allowing it to receive the data transmission it required from the hub. Through the iterations, the XBee was interfaced using either a straight connection in the RX/TX ports, or through a Lilypad XBee board. 

####Case

The clock’s hardware components are housed in a 3D printed case. As well as maintaining its security, this feature allows the clock to be seen more clearly; using a base to keep it upright. As a result, the clock can be viewed like most normal clocks, and from a multitude of angles in a room. The transparent covering holds the components in place, and optimises the appearance of the lights for viewing by using a frosting film. This was used to prevent glare from the LEDs, and to clearly distinguish one light from another. Since the clock is unique in appearance, it was necessary to develop a case which served its requirements and matched its specific dimensions. Towards the later iterations, the case underwent several changes, and was finalised with the above appearance.    

###Previous Work

####Iteration 1: Design and concepting phase
Since we have decided upon a web based method of visualising data, we want to explore several other means for outputting data to our clients. To engage more with the user, we want to explore methods of ambient data visualisation. 

From our bank of data visualisation ideas, the “clock” design incorporates the use of a coloured light system and a clock face display. The clock would receive the average noise level on the hour, and mark it on its face as a coloured segment. Over a 12 hour period, this would produce an overall chart of ambience. To get coverage over 24 hours, we would have two separate clocks displaying data for AM and PM. 

One option for this would be to involve the clock display in the website visualisation; offering the client a more ambient counterpart to graphs and charts. Since a few of our data visualisation for the five sketches phase featured a crossover of similar clock ideas, it is worth us looking further into how we could combine our sketches.  Since the clock design is simple, it could work quite effectively alongside more technical data formats. 

We could also implement something physical in hardware, which could be read similarly to a clock which tells the time. We discussed a few scenarios in which this would prove useful. Firstly, if a client returned home after being away from the house all day, a quick check of the clock could provide a straightforward spread of activity during their absence; drawing their attention to colours of greater intensity. 

In both cases, the red and orange lights could serve as flags to the user, as they indicate a high concentration of noise activity picked up by the sensor. Meanwhile, the lighter colours provide coverage for quieter hours. This offered the user a complete coverage of each 24-hour period, allowing the possibility to view previous hours and draw comparisons between them. 
Overall, we felt that there was an advantage to having a physical clock, as it is a physical artefact and therefore has greater potential to engage with the user. Furthermore, it could provide effective reinforcement to the data displayed on the web server. 

#####Lo-fi prototyping and sketches
In order to visualise a few different clock concepts, we created some sketches of the clock being represented in different ways. This was a way of gauging how the interface for the clock should look, as well as exploring some aspects such as colour, granularity, and the general visual layout for the user. 
Some of the design sketches we produced later in this iteration explored the placement of a clock as a “widget” on a tablet or webpage. Since our client could rely on mobile or tablet, we also concepted how our designs could be scaled and combined depending on where they are displayed. 

####What we need 
The premise behind the clock is to provide a visual stimulus to the user in a simplified data format for quick reference. It also needs to be interconnected with other devices on the network to accurately display data. Since the device is going to be running a series of lights, it is likely that power usage will be high; which is a consideration for how it will be powered in the user’s home. We also need to decide what sort of hardware we are looking at using to implement this idea, and what limitations and/or advantages each particular technology would give us. The main requirements of the clock are therefore: 
To provide an ambient visualisation of data which is engaging with the user 
Be easily implemented with the rest of the system 
Display data in a format that is easily grasped by the user
To have a feasible hardware solution which complements our other means of data visualisation

![Sketch 1](Images/Clock%20Images/IMAGE_1.png)

	Initial design sketches of the clock. Early iterations of the design featured a dual 12 hour display to represent AM and PM time notations respectively. 
	
<br>

![Sketch 2](Images/Clock%20Images/IMAGE_2.jpg)

	Cardboard prototyping of the clock, and sketches of data formats. The cardboard prototype uses coloured panels to represent hourly averages. Using these, we established a colour spectrum to use for the physical display. The right side image explores how averages are processed and visualised.  

<br>

![Sketch 3](Images/Clock%20Images/IMAGE_3.jpg)

	Exploring data continuity. The presence of the clock as a simplified data format complements the more technical aspects of the graphs and charts that we concepted. 

<br>

![Sketch 4](Images/Clock%20Images/IMAGE_4.jpg)

	Design sketch for clock display options. In this sketch, we looked at the block sizes for data; whether we would work in 10-30-60 minute intervals. We also considered how this format could also be translated into a webpage display.  
	
<br>

![Sketch 5](Images/Clock%20Images/IMAGE_5.jpg)

	Design sketch for the clock on a webpage, or tablet. Since we wanted the data available on different formats, we started to consider how we could scale different designs together. 

<br>

This device would in a sense act as an notifier to be coupled alongside the more detail-specific web server, and is designed to be more attention drawing. This was prototyped initially in lo-fidelity using card and coloured paper to simulate time segments, allowing us to explore how frequently data would be output to the clock. Through this prototyping phase, we began to develop our colour spectrum, and specifically how attention can be drawn to noisier time periods. As well as this, we considered whether updates would be formatted as 5/30/60 minute chunks, and how this could be replicated in hardware. 


####Outcome of Iteration
We have produced several different design proposals for the clock, and have began to consider how a physical device could be produced. We have evaluated what benefits the clock can bring to the project, in that it reinforces the more accurate data and makes for an engaging notifier for the user. There is also potential for this to create data continuity - in the sense that data from a physical device can be cross referenced with that on the web server. Overall, this iteration has set up the requirements for the clock, and given us a few interesting paths to take when developing it. 

####Iteration 2: Hardware and setup phase

After having collected our ideas from lo-fi prototyping, we are now exploring how our clock system can be physically implemented with hardware. 
The last iteration saw us investigating some requirements regarding the clock’s transition into hardware. This iteration involves our research into hardware, and comparisons between methods of implementing the system. 

#####Adafruit NeoPixel Ring

![clock models](Images/Clock%20Images/clock_models.png)

	Hardware models for the Adafruit NeoPixel Ring. Above, from left to right are the 24, 16 and 12 LED models. During the project we considered using various sizes, ultimately deciding upon the 24 x WS2812 (left). 

We were directed towards the tech solutions offered by Adafruit (https://www.adafruit.com) which distributes the NeoPixel product; an assortment of addressable miniature LEDs arranged in rings, strips and boards (https://www.adafruit.com/category/168). We feel that this is a good platform for developing our system in hardware.
The Adafruit NeoPixel device is a chainable collection of LEDs which can be interfaced with the NeoPixel Arduino library for support. Example code provided with the library demonstrated several of the device’s capabilities; notably the ability to manipulate timings of individual lights to come on. Conveniently, this was exactly what we were looking for as it provided a loose way of interfacing more complex tasks if we needed to. 

The setup of this device requires the use of a breadboard to interface the NeoPixel ring with an Arduino Uno microcontroller, and the use of a PC to upload code from the Arduino libraries to the Uno. After setup, our first concern was to establish the range of colours we could use to recreate our “ambience” colour spectrum.

We then displayed a spectrum of our proposed colours running from white (least ambient) through green, yellow, and orange to red (most ambient) recreating this colour wheel using the addressable LEDs. 

####Display Model
During our concepting of the clock display, we came up with several different possibilities of data accuracy that we could 
use. The main two directions for this were inspired by the 24 and 60 LED NeoPixel models respectively. 

#####24-Hour Display
The go-to approach that we looked into involved the 24-hour display clock, which averaged data over hourly periods, potentially transitioning to sleep mode between readings to save power. Adafruit’s solution to this offered an LED ring of 2.6” diameter; which is a relatively small display and could be mounted onto a small case or hub and unhooked for inspection. Early low fidelity prototypes of casing for this were modelled using cardboard, with emphasis being placed on the majority of the face being easily visible.  

#####Hour-By-Hour Display
There are also considerations to use other models of NeoPixel Ring. Of note is the NeoPixel Ring 60 x WS2812 model which offered 60 addressable LEDs; with which we can update more frequently to give an hour-by-hour display. A light would come on every minute after the averaged sample data was sent to the device until all 60 were lit; at which point it would rollover 
for the next hour. 

This idea could be combined with an LCD real time digital clock to keep track of which hour the data was referring to. This kind of display could also be interfaced with the clock itself to serve as the rollover point to be synched with the pixel 
reset at each hour. 

####NeoPixel Library and Code Iterations
After having chosen the NeoPixel 24 LED model, we have began to interface with the Arduino library to build a test program. 
From our test program, we intend to evaluate the capabilities of the NeoPixel Ring, and determine whether it could be used for our clock system. 

So far, we have found we can address individual LEDs and their brightnesses, and manipulate timings that certain lights come on. We are also looking into how we could implement our colour spectrum into the system, including the range of colours the hardware offers us. 

We knew the device would somehow have to read in data from our system. Writing a couple of test programs explained below, we require a data reading component to read in decibel values and convert them to a corresponding light
Reading clock data from a file
Initially, we constructed a program using the “Processing” IDE to work as a simple file reader which transmitted data via serial. Interfacing this with the microcontroller, a set of arbitrary values ranging from 0-9 were read into a switch statement on the arduino; 0 representing low ambience with a white light, and red expressing high ambience upwards towards 9.

![Neopixel Ring](Images/Clock%20Images/IMAGE_7.jpg)

	Adafruit NeoPixel Ring 24 x WS2812. Here the ring is powered by an Arduino Uno, and displaying a range of colours. The rings comes with adjustable brightness settings, and each LED is individually addressable. 

The code for this iteration can be found here: [Clock_Cycle_V1b]

#####Reading clock data from a pre-programmed array

Since the clock will be working with real values, we investigated the ways that we could test it without interaction of the real system. This would involve using dummy values, and setting 
A separate program we developed involved reading in dummy values from a pre-programmed array. The program would iterate through a size 24 integer array (consisting of integers from 0-100), and transfer data to a corresponding colour scale in the “setHourColour()” method. To fall within the case statements 0-10, each value was divided by 10 so that it would match a corresponding statement. The index positions of the array, which ranged from 0-23, matched each “hour” that the clock was displaying data for. This way, the clock display was to start from midnight, and iterate round to 11pm providing data coverage for all hours in between. 

The code for this iteration can be found here: [Clock_Cycle_V1b]

![setHourColour Code check](Images/Clock%20Images/IMAGE_8.png)

	Arduino code for “setHourColour()”. If the value exceeded -1 (which was always true unless an error case occurred - see below) then it gets divided by 10 to match the case statement. 

![Dummy code](Images/Clock%20Images/IMAGE_9.png)

	Arduino code for displaying “dummy” data lights. The loop() method simply iterates through the array of dummy values called “clockValues[]”, takes each individual value and converts it to a colour based upon which case it falls into. 

This simple setup will be the basis of a more complicated program, which will involve reading data in over serial, averaging values across an hour’s worth of data and turning on a light. This method of doing things was relatively straightforward to implement once we knew what form the data being transferred to the clock was in.    

For our data to be accurately represented, we had to somehow establish the colour spectrum which we developed in the initial design phase of the clock. Having looked into the NeoPixel library, the main method for handling the colour of individual LEDs was the “setPixelColor()” method, which took 4 parameters. Firstly, the position of the pixel you are addressing, and then RGB values for the colour. 
We determined our RGB parameters by using a HTML colour picker, which allowed us to pick and particular colour and read off the values. Using this, we established each of different parameters corresponding to the colours in our spectrum.  

![Colour picker](Images/Clock%20Images/IMAGE_10.png)

	We chose the colours using a HTML colour picker. Since the NeoPixel Ring method, “setPixelColor” requires RGB values for arguments, we determined which colour values to use based off of this website. (http://www.w3schools.com/colors/colors_picker.asp) 

####Simplifying the colour spectrum

Initially, we found the parameters for 10 different colours which would be used in progressively more intense hues. We will potentially reduce this to 6 different colours which provided the clock with better granularity. 

![Colour Spectrum 1](Images/Clock%20Images/Colour_spec_comparison.png)


	Simplified colour spectrum. Going in a clockwise direction, the colour converges more towards red, 	hinting at greater noise activity. The right image 	is displaying data for arbitrary values to test the 	granularity between colours.

The code for this iteration can be found here: [Clock_Cycle_V1b]

#####Colour-blindness spectrum 

There is an issue with using purely colour based visualisation. When catering for users who might have trouble distinguishing between colours, there is particular difficulty with telling the difference between green and red. Since our spectrum uses this two values as lower and upper bounds, it would be particularly problematic if the user couldn’t tell them apart. 

A method we could use is an intensity spectrum. Choosing one particular colour, the noise intensity would instead be represented by the intensity of each colour shade. For example, with red, quieter hours would be represented with very pale shades, and louder hours by more intense shades. 

![Colour intensity 1](Images/Clock%20Images/Intensity_spec_comparison.png)
		
	NeoPixel Ring displaying the intensity spectrum. 

#####Colour intensity spectrum. 

Similarly to the colour spectrum, the intensity increases as greater noise intensity values are read in. We have opted for red here, but it can be implemented in any colour.

We felt having the option to display data in these varying formats could be useful to the user, and opens up possibilities for new ways our visualisation could be implemented. There are potentially ways that these two formats could be used in conjunction.

The code for this iteration can be found here: [Clock_Cycle_RedIntensity]

#####Other Considerations

#####External 60-Minute Clock
A potential problem that we ran into when considering the design of the 24-hour display was the fact that for real-time data to be recorded, the client would have to wait for the next hour to tick over before it is reflected on the clock face. A proposed way of combatting this would be to have the larger, 60 LED NeoPixel running around the outside of the 24-Hour clock, reading in minute-by-minute data and updating just as often. The advantage here is that there are two dimensions of data for the client to access off the bat, and there is nothing stopping them from taking this data and using it to put forward a case straight away. Other features that might be useful in reinforcing this kind of display would be a real time clock interfacing with both pixel rings to control timings of readings, maybe even with an LCD display as a visual reminder so the client knows exactly upon quick reference which light applies to which minute. 

#####Controlling Brightness 

Fortunately as part of the NeoPixel Library, the ability to directly manipulate the brightness level of the entire strip is very straightforward. Since the levels range from barely visible to dazzlingly bright, it makes sense to control this to a user preference. It was decided that at nighttime, most users wouldn’t favour a bright, glaring light in their house. To control this, we looked at lowering the brightness gradually throughout the day, it being at its lowest (but still very visible) at night, and brighter in the day since it has to contest with other bright lights and sunlight. The clock could interface with a real time clock on the arduino to perform these checks every few hours. 


####Problems

We faced some issues in this iteration based upon our hardware choices. Although only used for prototyping, the Arduino Uno board has a lot of functionality which the clock doesn’t require, when ideally we want to condense down and use something simpler. Furthermore, dimensions wise the Uno is quite bulky - which may present further issues concerning the casing of the clock. Another problem concerned the starting point of the first LED that is lit. Since the NeoPixel Ring LEDs are addressed by the arduino code using integer index position, the starting LED, which remains fixed at 0, has been a current issue to change. If we can’t change the starting position, there could be potential problems regarding the orientation of the clock. For example, would we have to always have the clock at a fixed orientation? What would happen if it had to be rotated somehow? 

####Outcome of iteration
After this iteration was completed, we had managed to set up the NeoPixel ring to display some spoofed data values. This is promising as far as the full implementation of the clock goes, as we can now use this data format for structuring communications between the clock and other devices. Furthermore, we have decided upon a direction to take the hardware choices. Currently the clock is running off of an Arduino Uno as the NeoPixel ring is supported by Arduino code, but since it won’t be battery dependent, there is room for other microcontrollers to be considered. Also, this iteration has provided some interesting issues for us to consider; which we intend to solve in future iterations. 

####Iteration 3: Interlinking phase

####Adafruit Flora
To alleviate the issue of the Arduino Uno, we decided to change our board choice to the Adafruit Flora; a much more compact device. This significantly reduces the dimensions of the clock components, leading to a much more streamlined layout. This changeover will be particularly important for when we consider case design; during which a more condensed layout will be favoured. Furthermore, the Flora offers us the same functionality required of the Arduino Uno, minus the unnecessary features. 

####Interfacing with XBee and Networking
After we had successfully set the NeoPixel ring up to display some dummy values, and having decided upon a colour spectrum in our last iteration, our next task is to integrate the clock with the rest of the network.

The networking involved hooking up the XBee device to the Adafruit Flora microcontroller using the RX and TX ports, allowing the two devices to communicate over serial. The XBee was attached to an Lilypad board at this point, allowing for easy interfacing with the microcontroller. This was simply to keep the XBee in place. 

![Systems communication](Images/Clock%20Images/IMAGE_15.jpg)

	Hardware setup for clock. Above is the clock interfaced with the Adafruit Flora microcontroller, which, in turn is interfaced with the lilypad board.  

####Networking with the clock

To establish the clock device communicating over a network, we are going to start by writing simple sender-receiver code. This will be uploaded to the clock and a separate microcontroller - both of which will be  interfaced with XBees, so that messages can be sent back and forth between them. This is intended to simulate how clock would communicate with the hub, and pick up on any errors in transition that we may encounter. This will utilise a simple acknowledgement based format, which we could potentially use as foundation for how the clock requests hub data on the hour.

This sender-receiver code will then be implemented into the clock’s real interaction with the hub. The clock will send a request (formatted as “R:!”) which will then be accepted by the hub. The hub  then sends the data over in an integer array - which will be used to populate the clock’s “clockValues[]” array for storing data. 

The code for this iteration can be found here: [Receiver_Code]

![Systems communication](Images/Clock%20Images/IMAGE_16.jpg)

	Communication between the clock,  hub and web server. Once data from the web server has been sent to the clock, the clockValues[] array is populated with the values. 

![Clock hub communication](Images/Clock%20Images/IMAGE_17.png) - 
	
	Arduino code for the prototype clock-hub communication. This involved us establishing serial connections to the XBee device, and reading in values that were received from the sending XBee interfaced with the hub. The hub was simply configured to send the same pre-programmed array values over the network.

####Signposting with the clock
The clock, being a very visual component in the system, should implement some form of user signposting. Before the data display phase begins, we discussed adding a few intermediary stages to show the clock transitioning between states - if for some reason it has trouble reaching the data. 
IMAGE 18 - Clock display. This shows the clock reading in a value every minute from the hub, and plotting it as a light. The architecture pictured shows the NeoPixel ring, Adafruit Flora, and Lilypad interfaced together. 

![signposting flowchart](Images/Clock%20Images/IMAGE_19.png)

	Signposting flowchart. This chart shows that for every reading the clock takes on the hour, it runs the initialisation phase, and moves through the different signposts according to the state. 

We then planned what sort of signposting we might implement. This chart shows that for every reading the clock takes on the hour, it runs the initialisation phase. This phase consists of white lights circling round. After this, if data reading is successful, the first light is plotted. For subsequent readings, the appropriate states are reflected in the clock’s display. Before we implemented signposting, if there were errors in transmission of data, or communication to the hub, the clock would freeze its current display -  instead of reflect the nature of the problem. 

![Arduino error cases](Images/Clock%20Images/IMAGE_20.png)

	Arduino code for “error cases”. Here we introduced the specific cases which might occur. The comments clearly describe which case applies to which scenario. 

Once the modified case statement was developed, we tested the clock in isolation with specific error values, and observed the light output. This was to ensure that the signposting was effective, and clearly conveyed different messages. 

![case 1](Images/Clock%20Images/IMAGE_21.jpg)

	Display for error case 1. The clock outputs a white display of lights and holds it there for a few seconds. 
	

![case 2](Images/Clock%20Images/IMAGE_22.jpg)

	Display for error case 2 - if the hub cannot be reached. The clock outputs a red “colour wheel” of light to indicate that there is an error. 

![case 3](Images/Clock%20Images/IMAGE_23.jpg)
	
	Display for error case 3 - if the hub cannot be reach the server. The clock outputs a blue “colour wheel” of light to indicate that there is a problem.
	
![success 1](Images/Clock%20Images/accept_case.jpg)
	
	Display for success case - if the hub can successfully be reached by the clock, the green lights appear. After this, the coloured data appears.
	
The above cases show the clock displaying errors whilst working with dummy values, so our next stage is to make it work with the hub. This is fairly straightforward - we just have to send dummy data packets from the hub over to the clock over the Zigbee network to test it.

The code for this iteration can be found here: [Clock_Cycle_V1b] 

At this point, much of the clock’s functionality is working as intended. So far, we have tested the clock in isolation, and simulated its interactions with the hub device. We have also established a communication platform from our sender-receiver code, which we will adapt to integrate the clock into the system. 

The code for this iteration can be found here: [Clock_Cycle_V1]

####Integrating the clock with the whole system

Now that we have developed the main functionality of the clock, we have to integrate it with the rest of the system. To do this, we must carefully optimise a few of the clock’s features - such as wait times in between reading values; in order to synchronise it with other system events such as the sensor reading in data, or the hub communicating with the web server. This process involves refactoring some of the arduino code for efficiency reasons, and beginning to consider how we could use protective casing when it came to deploying the clock. 

Since the data format for the clock has already been decided upon, the integration of it into the system is more a matter for the other components. Here, there were discussions about how the web server could send packets tailored specifically for the clock via serial, whilst maintaining the format that the clock expects.

The first integration step was to adjust delay times: 

![delay code](Images/Clock%20Images/IMAGE_24.png)

	Arduino code for delays. The variables represent different lengths of time that we could delay the clock for between reading values, as determined by the number of miliseconds. 

The clock is designed to display average readings on an hourly basis. For the sake of testing its functionality over a shorter timescale, we are running the clock in shorter iterations; e.g. minute by minute displays (for a total of 24 minutes) and second by second delays (24 seconds). This reduced time whilst testing, and saved having to wait 24 hours just to see the output of a day’s data. 

There are currently further plans to test the clock over an extended time period, once it has been fully integrated with the whole system.

The code for this iteration can be found here: [Clock_Cycle_V2]

####Video Explanation

The interaction between the clock and the hub has been recorded in a video, and can be seen <a href = "https://www.youtube.com/watch?v=sQJQwxEeyLs&ab_channel=BillyMitchell">here.</a>
		
		The video above explains how the signposting for the user works, and generally gives an overview of how the clock communicates with the hub to plot its data.


![Clock network diagram](Images/Clock%20Images/Clock_network_diagram.png)

		By the end of this iteration, the communication between the clock and the hub resembled this diagram.

####Previous Problems

<ul>
<li> Arduino Uno board has a lot of functionality which the clock doesn’t require, when ideally we want to condense down and use something simpler. </li>
<li> Starting point of the first LED that is lit. Since the NeoPixel Ring LEDs are addressed by the arduino code using integer index position, the starting LED, which remains fixed at 0, has been a current issue to change. </li>
<li> Lack of error displays if e.g. the clock can’t communicate with the hub, data can’t be transmitted etc.</li>
</ul>

Early in this iteration, we moved from our choice of board for prototyping; the Arduino Uno, over to a more concrete choice. The Adafruit Flora currently fits our needs for a stable, yet compact board, and has the added bonus of fitting inside the NeoPixel Ring. The Flora is compatible with our previous Arduino code, and has exactly the ports we need for interfacing with the NeoPixel Ring. This makes it a more efficient, and aesthetically suitable choice for the clock. 

We resolved the issue of the starting point by making a very simple adjustment to our code. For any iterative structures looping over the LEDs, rather than starting at position 0 (the first LED), we simply offset that number by an amount depending upon where we wish to start (- e.g. + 3 if we wanted to start from the third LED going clockwise), storing that in the “startpos” variable. When it came to iterating from this new start position, we iterated over the LEDs as normal, calculating the shift in LED indexes. (see code below).

To add a degree of error displays, we added a new light system for the clock to provide signposting. These operated similarly to other devices which use warning lights, and intended to direct user attention based on colour. 


![case 1](Images/Clock%20Images/IMAGE_25.png)

	LED index changes


####Current Problems

During this iteration, some of the main challenges concerned setting up the clock on the network, and maintaining the format of the data it received so as to not trigger errors. The clock uses a very specific format in which it expects data, so a particular challenge has been in normalising the structure of packets to match this expectation. Fortunately, however, if there are genuine issues regarding the web server’s connection to the hub, or the hub’s communication with the clock, we have established signposting to inform the user. 

####Outcome of iteration

After this iteration was completed, we had managed to set up the NeoPixel ring to display some pre-calculated data values. This is promising as far as the full implementation of the clock goes, as we can now use this data format for structuring communications between the clock and other devices. Furthermore, we have decided upon a direction to take the hardware choices. Currently the clock is running off of an Arduino Uno as the NeoPixel ring is supported by Arduino code, but since it won’t be battery dependent, there is room for other microcontrollers to be considered. Also, this iteration has provided some interesting issues for us to consider; which we intend to solve in future iterations.

####Iteration 4: Casing and finalisation phase

In the previous the functionality of the clock was integrated with the rest of the system in the previous iteration, 
Since the NeoPixel ring by itself is quite a fragile piece of hardware, it is necessary to build some form of casing to protect it whilst still maintaining its visibility. We came up with a few ideas for this using cardboard to easily mould shapes together; most of the principle designs consisting of a base and a clear perspex glass “face”, to protect the components. 

We also looked into how the device could be mounted, since ideally our focus was on its attention drawing aspect which would be less effective if it had to be picked up or consciously interacted with. This could be achieved with a clip or some form of hook for a wall mount possibility, or perhaps a subtle stand so as to not rely too much on modifying a client’s home to accommodate for it. 

####Designing the case

When it came to designing the case to house the clock, we have several things to bear in mind: 
<ul>
<li> It has to be robust enough to protect the hardware</li>
<li> Easy to mount and/or stand up</li>
<li> Of reasonable size</li> 
<li> Good visibility of the clock display</li>
<li> Feasible to manufacture/3D print</li>
</ul>
With these points, we set about producing some design mockups using google sketchup. Since we weren’t looking at modelling straight away, it was good to use this program to conceptualise some of properties of its general appearance.

#####Iteration 1


![Design 1 a](Images/Clock%20Images/IMAGE_26.jpg) ![Design 1 b](Images/Clock%20Images/IMAGE_27.jpg)

	Sketchup designs of first prototypes. The general shape of the case is established here. The isometric view on the left shows the layers where the NeoPixel Ring could sit.

In this iteration we designed the general shape of the case, and considered a transparent perspex “face” to promote the visibility of the lights. It was designed so the NeoPixel Ring would fit securely around the inner ring, with a space in the middle for the Flora to sit. The purpose of the base here is twofold. Whilst it doesn’t yet stand up, the base holds the components for the XBee module, and features a detachable back.  

#####Iteration 2

![Design 2 a](Images/Clock%20Images/IMAGE_28.jpg) ![Design 2 b](Images/Clock%20Images/IMAGE_29.jpg)

	Iteration 2. Here we began to establish how the case could use a stand to support the clock, and make the viewing angle easier. 

This iteration features our first implementation of a stand. This is important to the evolution of the clock, as it greatly increases visibility of the lights, and therefore aids the user’s engagement with it. The stand here is fairly small, so in future design iterations we will have to consider making more robust changes.

#####Iteration 3

![Design 3 a](Images/Clock%20Images/IMAGE_30.jpg) ![Design 3 b](Images/Clock%20Images/IMAGE_31.jpg)

	Iteration 3. This design uses a pull out stand which would potentially provide greater stability than the stand in the previous iteration. This would also be a useful feature for wall mounting the clock. 

This iteration implements a more sturdy stand to keep the clock upright. We were considering having a flip out stand, which can be put away based on user preference. Besides that, much of the general design is the same. 

#####Final iteration

We finalised our case design to have a more sturdy base and used the 3D printer to construct it. The final case consisted of three major 3D printed parts which slotted together.

The final case design was constructed from white plastic; featuring a clear plastic face, a sturdy base with which it could stand upright, and a container for the Adafruit Flora and the XBee. Overall we felt this case suited the clock very well, matching its dimensions accurately and providing a nice aesthetic. The face importantly provided a clear view of the lights, and offered granularity between each individual light. 

![Design final a](Images/Clock%20Images/final_1.jpg)
 	
 	Final design for the clock case. 
 	
![Design final b](Images/Clock%20Images/final_2.jpg) 

	Final design for the clock case, side view. 

For more information, see casing. 

####Previous Problems
<ul>
<li>Setting up the clock on the network, and maintaining the format of the data it received so as to not trigger errors. 
</ul>
We solved this issue this iteration by incorporating a strict data format that the clock received values in. This was an integer array of 24 values, containing sound averages for each hour of the day. This format was utilised by the hub; meaning that when sending data, the clock received it in a format it “understood”. 

As a result, the clock is now synchronised with the rest of the system, and can effectively notify users with up to date information, which is displayed accordingly. 

####Outcome of iteration
At the end of this iteration, we had successfully developed a case for the clock; precisely matching its dimensions and storing all the necessary components. In essence, the functionality for the clock was complete, and had become a fully integrated part of the system.




### Minutes

Below are the minutes taken for every meeting that we, personally had between us as a group. They are a rough summary of what decisions we chose to take and how we followed up on them in the weeks after.

Weekly Minutes (September 27th - 25th March 2016)

####27/9/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Project begins.
* Meeting with supervisor, arranging a meeting with clients.
* Research into general hardware understanding.
* Research into general electronics understanding.

##### Work Achieved previous week
* Orange Street itself documented, photographs taken of popular outlets along the street and general overview.

####3/10/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Research into sensor amplifier, this is required for our sensor to accurately measure noise.
* Reading Portigal Book, this is required to interview our clients in the best format possible.

##### Work Achieved previous week
* Circuit diagrams for amplifier and research completed.
* Research into hardware boards completed.

####10/10/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride

##### Main points
* Working on sensor amplifier, have to calculate accurate values for use.
* Concluding on client meetings, taking notes from the important moments in the meeting.
* Sensors are required as expected, multiple of them will be used to gather data on sound.
* Starting to learn 3D print, will need case designs for components in the project. 
* Start researching potential solutions in system architecture to the problem.

##### Work Achieved previous week
* Client interviews completed, recorded and documented. 

####18/10/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* A Basic sensor has been created using the amplifier circuit, can now use this act as a sensor temporarily.
* Program needs to be written to sample sound from the current sensor.
* Researching sound waves and understanding of sampling, need a better technical understanding of sampling sound waves and sound in general.
* Researching into potential networking solutions, we need a way of transferring this data from the sensor to the website.
* Researching into hardware boards, Arduinos, MBEDs, any particular board that could be used to handle the sensors requirements.

##### Work Achieved previous week
* Microphone amplifier built, testing begins this week.
* System architecture researched, deciding to use - sensor to hub to webserver.
* 3D print attempts, not successful for the board - but very close.
* Contacted council and received advice for legal standing on our project.

####25/10/2015
##### Members attendance
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Using the written program for sampling data from the sensor, taking this sampled data and working out how to gather the values we need in order to demonstrate a sound level.
* Using the FRDM K64F as a guide - 3D printing a case for this board.
* Decision to use the XBee as our networking module, offers great customisation and low power cost.

##### Work Achieved previous week
* Created program to sample sound waves, tested on Arduino and FRDM-K64F boards.
* First successful 3D printed case designed

####2/11/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Started using Github to handle different elements of the project code base.
* Started getting XBees S2 talking to each other, sending dummy data between two modules.
* Began discussing how often we wish to sample data, how many samples to take and the accuracy of our data. 
* Researching into very low boards to act in place of the sensor.

##### Work Achieved previous week
* Displaying sound a period of time, research needed on decibels.
* Plenty of test data stored from the initial testing of sensor.
* Research completed on networking solutions.
* Website API implemented, sensor iteration 4 started by Dominic.

####9/11/2015
##### Members attendance
* Daniel Andrews
* Jack McBride
* Matthew Aldridge

##### Main points
* Discussed the potential of using the FRDM-K64F as a Hub as we are familiar with it.
* Looking into XBees and their configurability, discussing whether encryption or the alike is necessary.
* Researching into converting sound values into decibel levels.

##### Work Achieved previous week
* Using XBees to communicate over serial on FRDM-K64F with MBED application shields with AT mode.
* Concluded to sample data once a minute for 3 seconds, 60 times an hour.
* Multiple case designs printed.

####16/11/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Still working with XBees, looking into setting up multiple on a network with a mesh topology as opposed to point to point.
* Discussed the battery concerns, clients ideally want a rechargeable set of batteries. Looking into battery solutions.
* Sketching visualisation ideas.  
* Decided on using the Rocket scream board for the sensor, but going to use Arduino Uno for the time being.

##### Work Achieved previous week
* Decided to use the FRDM-K64F as the Hub
* Successfully found a way to convert raw values from the sensor into decibels.
* Looked into configuring XBee modules.
* Sensor implemented clock for timestamping.

####23/11/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Matthew Aldridge

##### Main points
* Attempting to get an Arduino board communicating to an XBee, wiring the module up ourselves as it’s likely we’ll use an Arduino board.
* After showing 5 sketches produced from each other, we concepted some form of clock that displays noise levels.
* Researching into different solutions for a Hub instead of FRDM-K64F. 
* Prototype of sensor is going to be placed in client's house over Christmas break.
* Prototype will use local SD card instead of network due to power restrictions with the board being used.

##### Work Achieved previous week
* Researched batteries, worked out calculations required for the sensor in its current state to function over Christmas.
* Concepted the idea of a ‘clock’ from 5 sketches idea.
* Rocket Scream ordered, waiting for arrival.

####30/11/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Mapping locations for sensors, where are we likely place them - what sort of problems does this raise?
* Concepting case designs for the Arduino Uno.
* Researching components for a clock like device.
* Working towards placing a sensor in client's house before Christmas.

##### Work Achieved previous week
* Arduino board communicating with XBee and an FRDM-K64F over serial using AT mode.
* Sensor SD card implemented, data will be stored locally for the project due

####7/12/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Going to use AT mode for XBee for Hub and sensor, simpler to set up and then focus on API mode of XBee. 
Research into XBee settings, API mode.
* Clock components decided, visualising components and how to use them. Going to use 24 LEDS on the clock for 24 hours.

##### Work Achieved previous week
* Case for sensor printed.

####14/12/2015
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Clock testing and programming, deciding what colours to use, frequency of them.
* Case design tested, researching into weatherproofing the case.
* Raspberry Pi decided for the Hub.
* Going to use a large battery to power sensor for 8 days roughly.
* Decided to sample once a minute and average to use as a representation of that minute.

##### Work Achieved previous week
* Components arrived for clock, connected together and testing 
* Testing on sensor.

####21/12/2015
##### Members attendance
* Daniel Andrews
* Jack McBride
* Matthew Aldridge

##### Main points
* Final meeting before Christmas break, sensor has been placed in Client's house on Orange Street. 
* Rocket scream board arrived, testing with rechargeable batteries is next step
* AT Networking finalised.

##### Work Achieved previous week
* Sensor installed in client’s house
* End of term 1.

####20/1/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* First meeting since end of Christmas break, sensor has been collected and data returned for evaluation.
* From result it is hard to determine accuracy of noise, sampling needs to be more accurate
* Increasing sampling rate to 3 times a minute.
* Work commencing on creating the Hub, and arranging networked solutions.

##### Work Achieved previous week
* Over Christmas, research on Rocket Scream, XBee, Clock and Case design.

####27/1/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Matthew Aldridge

##### Main points
* Clock case design prototyping started.
* Raspberry Pi Model B+ acquired, programming beginning in Python with Jessie Lite as the Operating System.
* Investigating how to visualise data on the website.
* Order requested for components to build more sensors.
* Sampling rate is now much more accurate.

##### Work Achieved previous week
* Raspberry Pi initialisation, setting up and configuring to work with a XBee.
* Rocket Scream replaced Arduino Uno as sensor board.
* Sample rate modified to be more accurate.
* Initial test of Clock with 24 LEDs, potentially using 60 LEDs.

####3/2/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Programming the RocketScream, research into power usage with the board (disabling/enabling features).
* Researching sleep mode configuration on XBee modules.
* Case design for Hub started.
* Hub to backup data if network fails.

##### Work Achieved previous week
* Pi configured to work as coordinator, work commencing to program.
* Initial version of Hub AT model finished.
* Case research into waterproofing, protection against rain in particular.

####10/2/2016
##### Members attendance
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Finished hub, need to test with all components under different circumstances.
* Sensor finished, accuracy lacking - looking into solutions.
* Case design for clock on-going.
* Colour sensitive users for clock.

##### Work Achieved previous week
* Case design for clock finished along with initial sensor.
* Initial Clock program finished, interfacing with Hub is next step.

####17/2/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Light intensity as opposed to different colours for users of colour blind nature.
* New sensor prototype almost finished, solution to accuracy is a 16bit ADC.
* New case design for sensor, directional microphone.

##### Work Achieved previous week
* Website configured to communicate with Hub on GET and POST.
* Hub tests all successful.
* Clock case prototyped.

####24/2/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Testing entire system in the ‘wild’, sensor outside, clock on the side and Hub routing traffic.
* If goes to plan, place in Client's house during this week.
* Case finished for sensor, case for clock next.
* Current networking is AT mode.
* Live visualization being worked on.

##### Work Achieved previous week
* Decision to work with API mode has been made.
* Sensor now uses 16ADC as well to boost accuracy of raw values.
* Clock features colour alternatives for those who suffer from colour blindness.


####31/2/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Designing initial poster for project fair.
* Programming API mode for network.
* Design for clock case finished. Need a way to diffuse light.

##### Work Achieved previous week
* System tested in our own homes, it even snowed - perfect for testing the sensor.
* Live visualisation completed, working on preparing the project for live use.

####7/3/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride

##### Main points
* Placing system in clients home.
* Planning on testing website with users.
* Hub API mode finished.

##### Work Achieved previous week
* Poster concepted and designed.
* Clock case finished and light diffused.
* Hub API mode finished.


####14/3/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Building of a dummy sensor working with API mode to demonstrate capabilities.
* Visualisation of data finished.
* Clock case finished.
* Plans for poster to test the clock and demonstrate our visualisation.

##### Work Achieved previous week
* Poster concepted and designed.
* Clock case finished and light diffused.
* Hub API mode finished.

####18/3/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Use clock to demonstrate noise levels and visualisation.
* Laptops and tablets for visualising website.
* Dummy sensor to demonstrate range and error correction.
* Hand out flyers on the project.

##### Work Achieved previous week
* Poster concepted and designed.
* Clock case finished and light diffused.
* Hub API mode finished.

####25/3/2016
##### Members attendance
* Daniel Andrews
* Dominic O’Connor
* Jack McBride
* Matthew Aldridge

##### Main points
* Formating corpus and technical report for project.

##### Work Achieved previous week
* Dummy sensor was a success.
* Live visualisation was a success.
* Sensor has survived for 2 weeks in the wild, including conditions.
* Clock updating correctly.
* Won the poster fair. 
* We managed to get rid of all our flyers.
##Sketches

To generate and combine ideas for our forms of data visualisation, we individually produced sketches. These sketches consisted of different formats of visualisation; ranging from the more technical approach of graphs and charts, to ambient forms of colour systems and "heatmaps".  

###Iteration 1

####Five-Sketches-or-Else Method

Our initial sketching phase, the **"5 sketches or else"** method, involved us each individually producing 5 different sketches of our proposed data visualisation. This was to be done in isolation of one another, preventing individual ideas from clashing with the opinions of others, and to promote unique designs amongst group members. 

We set out to design easy, straight forward ways to show data that our sensors had gathered. This premise inspired us to produce simple designs which weren't over complicated, but also ones that weren't limited by specific requirements. This allowed us to express some creative freedom in the process, and produce some interesting designs. 

**Here are our outcomes of the Five-Sketches-or-Else Method:** 

These initial sketches were drawn in pencil, and explored an open ended approach to data visualisation. They were comprised of several unique ideas; varying from typical graph formats, to looking at specific formatting for devices and tablets. 



![sketch 2](Images/Sketch%20Images/Matt_2.jpeg)

	Sketch 1 - Sensor visualisation. This design focuses on highlighting the location that was most noisy. This was based upon the position of the dot, providing coverage over the course of a day. 
	
![sketch 1](Images/Sketch%20Images/Matt_1.jpeg)
	
	Sketch 2 - Scatter graph sensor visualisation. This provides a more technical overview, plotting data of specific volume values for each sensor.

![sketch 3](Images/Sketch%20Images/Matt_3.jpeg)

	Sketch 3 - Unique approach to graphing noise volume against time.
 
![sketch 4](Images/Sketch%20Images/Matt_4.jpeg)

	Sketch 4 - 3D graph view. 

![sketch 5](Images/Sketch%20Images/Matt_5.jpeg)

	Sketch 5 - Data visualisation on iOS devices.

<br>

From these sketches onwards, more of an emphasis to colour visualisation is made, drawing inspiration from colour systems to convey data. 

![sketch 6](Images/Sketch%20Images/Jack_1.png)
	
	Sketch 6 - Simple daily bar chart view

![sketch 7](Images/Sketch%20Images/Jack_2.png)

	Sketch 7 - Clock display. This is a more novel approach to data visualisation, and uses a clock to plot sound averages as segments. 

![sketch 8](Images/Sketch%20Images/Jack_3.png)

	Sketch 8 - Monthly sound average view. 

![sketch 9](Images/Sketch%20Images/Jack_4.png)

	Sketch 9 - Side by side view of critical noise periods against normal periods.

![sketch 10](Images/Sketch%20Images/Jack_5.png)

	Sketch 10 - Interchangeable bar chart and line graph view. Offers two different views of data, which can be altered according to user preference.

![sketch 11](Images/Sketch%20Images/Jack_6.png)

	Sketch 11 - Line graph plot of average noise, current noise, and previous month's average.

![sketch 12](Images/Sketch%20Images/Jack_7.png)

	Sketch 12 - Histogram view of normal sound levels vs current sound levels.

![sketch 13](Images/Sketch%20Images/Jack_8.png)

	Sketch 13 - Heatmap view - another novel form of visualisation, this is a map of Orange street which is coloured according to noise "hotspots". Used to provide granularity between quieter and noisier areas, and pinpoint disturbances.

![sketch 14](Images/Sketch%20Images/Jack_9.png)

	Sketch 14 - Line graph and colour coded display - intended to establish sound "thresholds", which clearly indicate to the user when the noise level has crossed a certain boundary. 

![sketch 15](Images/Sketch%20Images/Jack_10.png)

	Sketch 15 - Calendar view - A combination of the clock view with a daily/weekly/monthly comparison chart. 
	
![sketch 16](Images/Sketch%20Images/Dom_1.jpeg)

	Sketch 16 - Chart and graph view - this is a crossover design featuring a min and max for sound readings, and a date layout in a chart. 

![sketch 17](Images/Sketch%20Images/Dom_2.jpeg)

	Sketch 17 - Sensor scatter graph - very similar to one of the other concepts, this design showed how there were common elements between our work even though they were completed in isolation of one another. 

![sketch 18](Images/Sketch%20Images/Dom_3.jpeg)

	Sketch 18 - Clock view - this sketch reinforces the time element of the monitoring, and incorporates ambient colour. 

![sketch 19](Images/Sketch%20Images/Dom_4.jpeg)

	Sketch 19 - Threshold bar chart - a combination of a bar chart with intervals representing sound thresholds. It features added labels to associate the coloured threshold with a corresponding noise range, e.g. street, nighttime.

![sketch 20](Images/Sketch%20Images/Dom_5.jpeg)

	Sketch 20 - Coloured chart - this combines a less technical overview of data; presenting the high, low, and average decibels to the user. It also uses a colour system to direct the user towards louder time periods, by using more intense colours. 



<br>

####Outcome of Iteration
During this iteration, we began to develop our data visualisation platform. We considered individual approaches to visualising sound data; incorporating trusted forms of display such as graphs, diagrams, and chart, as well as experimenting with new, less common ideas. As a result of these experiments, we approached our data visualisation creatively, creating a wide range of possibilities for what we finally provide to our client. 

After this iteration, we have set up a platform to build upon for our data visualisation. We can now look into combining ideas to come up with more dynamic concepts. 

####Iteration 2

####Combined Sketches 
Once we had each individually developed five sketches, we shared them, and collaborated ideas behind our chosen data formats. This lead to crossovers in ideas, and common themes and designs behind ideas were identified. 

In this phase, we focused on combining our ideas and extending the functionality of our existing designs. As a result, our sketches became more specific, and we began to consider how realistically they could be implemented. 

The focus here, was placed upon what forms of visualisation the client would find most useful. We held onto some of our novel ideas, such as **the clock**, and further developed them alongside other means. 

**Here are our outcomes of the Combined Sketches Method:**

![combined sketch 1](Images/Sketch%20Images/Matt_comb_1.jpeg)

	Combined sketch 1 - Histogram crossed with noise threshold.

![combined sketch 2](Images/Sketch%20Images/Matt_comb_2.jpeg)

	Combined sketch 2 - Graph and clock view - incorporates a clock view to display user, and shows the relative sound level at each time interval.

![combined sketch 3](Images/Sketch%20Images/Matt_comb_3.jpeg)

	Combined sketch 3 - Sensor and decibel level view - combines the ideas of having a sensor centric visualisation which pinpoints activity, and a decibel scaling system.

![combined sketch 4](Images/Sketch%20Images/Jack_comb_1.png)

	Combined sketch 4 - Heatmap and graph visualisation - this sketch integrates the novel approach of the street heatmap with a line graph to reinforce the data.

![combined sketch 5](Images/Sketch%20Images/Jack_comb_2.png)

	Combined sketch 5 - Threshold graph comparison - designed to give the user to ability to compare several different dates in terms of sound thresholds that were crossed. 

![combined sketch 6](Images/Sketch%20Images/Jack_comb_3.png)

	Combined sketch 6 - Touch screen / tablet heatmap view. Adds the extra detail of a colour based averaging system to the heatmap, and implements it for a touchscreen device. 

![combined sketch 7](Images/Sketch%20Images/Dan_1.png)

	Combined sketch 7 - Graphical sensor comparison. This visualisation provides a time scale on the X axis, and the ability to add series from different sensors. 

![combined sketch 8](Images/Sketch%20Images/Dan_2.png)

	Combined sketch 8 - Sensor map view - incorporates a variable time system with a sensor map, where the user can view sensor activity according to their desired time parameters. 

![combined sketch 9](Images/Sketch%20Images/Dan_3.png)

	Combined sketch 9 - Coloured time chart - a colourful approach to visualisation, this sketch creates a coloured sound scaling system, and plots its as a grid. Useful for daily comparisons, and averaging sound activity over time in easily interpreted formats. 

####Outcome of Iteration
During our second iteration of sketching, we have developed upon our existing bank of data visualisation ideas, and been more conscious in our approach of realistic data formats. This iteration has featured a much more colour oriented approach; drawing more upon ambient visual aids to create signposting for the user. The fidelity of the designs has resultantly increased, and ideas have become more amalgamated together, and specific to the client's needs. 

A recurring theme for visualisation during this iteration has been time, and how our system can use it to generate comparable data. It is concluded that time plays a large role in our system; especially since one of the major requirements is to have comparable data between dates. 

###Iteration 3

####Balsamiq, and Interface Mockups

Before we implemented our front end visualisation, it was important to explore different layouts and styles, which could then be translated into code. To do this, we used the lightweight mockup tool, **Balsamiq**, the create several different design mockups. These mockups were intended to simulate our various data platforms, by visualising website and application layouts, and having charts and diagrams to show how our visualisation would work in these contexts. 

In these designs, we aimed to produce easily interpretable interfaces which focus purely on visualisation. We wanted to remove any other distractions from the user, and draw their attention to the main priority: their data. 

To do this, our designs are minimalistic, and implement easily graspable layouts and methods of navigation. There is opportunity to further develop upon these simplistic designs at later points, but these designs capture the essential details. 

**Here are the outcomes of our Balsamiq designs:**

![Balsamiq sketch 1](Images/Sketch%20Images/Balsamiq_1.png)

	Balsamiq Sketch 1 - Main street view of Orange street. This is intended to be the main interface for the front end visualisation, where the user can add data widgets such as the "graph" and "calendar" to build the data they want to view. 


![Balsamiq sketch 2](Images/Sketch%20Images/Balsamiq_2.png)

	Balsamiq Sketch 2 - Specific data view for sensors - the user can click or touch a sensor and access the data for that specific sensor. 

![Balsamiq sketch 3](Images/Sketch%20Images/Balsamiq_3.png)

	Balsamiq Sketch 3 - Graph view - here the earlier graph sketches are represented on a website, featuring a date widget that the user can manipulate to return data from different time periods.


![Balsamiq sketch 4](Images/Sketch%20Images/Balsamiq_4.png)

	Balsamiq Sketch 4 - Graph and calendar comparison widgets - having activated both the graph and calendar widgets. The user can compare two sets of data over different time periods, side by side. 


![Balsamiq sketch 5](Images/Sketch%20Images/Balsamiq_5.png)

	Balsamiq Sketch 5 - Single graph and calendar widget - if the user wants to add different dates as series on a graph, this view meets their requirement. Extra data series can be added using the "Add data" button.


![Balsamiq sketch 6](Images/Sketch%20Images/Balsamiq_6.png)

	Balsamiq Sketch 6 - Map and graph comparison widgets - If the user wants to view general sensor data and compare it to average hourly data, this view can be used. Also featured is the scaling icons, which the user can use to increase, or decrease the size of their widgets; depending upon what they want to see more. 


![Balsamiq sketch 7](Images/Sketch%20Images/Balsamiq_7.png)

	Balsamiq Sketch 7 - 4 widget view - the complete package of data widgets, granting data continuity to the user across four separate platforms.


![Balsamiq sketch 8](Images/Sketch%20Images/Balsamiq_8.png)

	Balsamiq Sketch 8 - Tablet/mobile view - if the user prefers using other platforms, this sketch demonstrates how the interface might scale depending upon the size it operates in. 


![Balsamiq sketch 9](Images/Sketch%20Images/Balsamiq_9.png)
	
	Balsamiq Sketch 9 - Hub interface display - later in this stage, we considered visualisation for the hub, and how a diagnostics system could work. Since it is the centrepoint of the network, this could be used to display information to the user based on which devices are in range, and specific properties of them. 
	

####Outcome of Iteration
In our final iteration of sketching and mockups, we have successfully established  a set of design blueprints which can be used to drive our front end visualisation. These designs implement some of the key concepts seen in our earliest form of visualisation, and develops them in a high fidelity manner. This process has added a degree of realness and feasibility to our designs, by effectively filtering out the less reliable forms of visualisation, and drawing the user's attention to more quantitative aspects. These forms of data, implemented as graphs and charts, display sound data in a technical way, and can be used in conjunction of one another to build a holistic description of the sound levels. 

Overall, we felt that the sketching process helped us formulate and refine our ideas, and that over our sequence of iterations there have been continual improvements. Each iteration has marked a significant development in our ideas, as they have become more specific and considerate of the user over time. 



   
